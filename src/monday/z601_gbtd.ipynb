{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vd-Rfyik62j"
      },
      "source": [
        "# Gradient Boosting Desicion Tree\n",
        "\n",
        "En las clases anteriores, observamos cómo las mejoras en los algoritmos y las optimizaciones pueden generar avances significativos en la ganancia. Ya hemos logrado un progreso considerable con los modelos de Random Forest. Hoy, daremos un paso aún más grande al explorar los modelos que actualmente están obteniendo los mejores resultados en este tipo de dominios.\n",
        "\n",
        "Antes que nada, carguemos el entorno de trabajo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhhu79HVkwb5"
      },
      "outputs": [],
      "source": [
        "#%pip install scikit-learn==1.3.2\n",
        "#%pip install seaborn==0.13.1\n",
        "#%pip install numpy==1.26.4\n",
        "#%pip install matplotlib==3.7.1\n",
        "#%pip install pandas==2.1.4\n",
        "#%pip install lightgbm==4.4.0\n",
        "#%pip install optuna==3.6.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj-rL6xHlA2u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "import optuna\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_slice, plot_contour\n",
        "\n",
        "from time import time\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8jGKjoN1lRho"
      },
      "outputs": [],
      "source": [
        "base_path = 'C:/Users/c678456/Desktop/Ian/Maestría/Especializacion/2do_cuatrimestre/DMEyF/'\n",
        "dataset_file = 'datasets/competencia_01.csv'\n",
        "modelos_path = base_path + 'modelos/'\n",
        "db_path = base_path + 'db/'\n",
        "\n",
        "ganancia_acierto = 273000\n",
        "costo_estimulo = 7000\n",
        "\n",
        "mes_train = 202102\n",
        "mes_test = 202104\n",
        "\n",
        "# agregue sus semillas\n",
        "semillas = [211777, 174989, 131497, 612223, 234803]\n",
        "\n",
        "data = pd.read_csv(base_path + dataset_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TrH9f1L5Umd"
      },
      "source": [
        "Vamos a asignar pesos a las clases. En unos minutos explicaremos las razones detrás de esta decisión. Mientras tanto, pueden aprovechar el código para ajustar el peso de la clase **BAJA+2** según lo deseen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AlYeDIBQP3-s"
      },
      "outputs": [],
      "source": [
        "data['clase_peso'] = 1.0\n",
        "\n",
        "data.loc[data['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
        "data.loc[data['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzLIRVs850-I"
      },
      "source": [
        "Además, como se mencionó en la clase pasada, comenzaremos a experimentar con nuevas clases para ajustar el modelo. En particular, sumaremos la clase **BAJA+1**, que es estructuralmente muy similar a **BAJA+2**, para aumentar los casos positivos. Luego, compararemos los resultados obtenidos con los de la clase con la que hemos estado trabajando hasta ahora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KV1meQ5cZ_Sl"
      },
      "outputs": [],
      "source": [
        "data['clase_binaria1'] = 0\n",
        "data['clase_binaria2'] = 0\n",
        "data['clase_binaria1'] = np.where(data['clase_ternaria'] == 'BAJA+2', 1, 0)\n",
        "data['clase_binaria2'] = np.where(data['clase_ternaria'] == 'CONTINUA', 0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AYJG0r16dW9"
      },
      "source": [
        "Y trabajaremos como es habitual en las últimas clases, con **Febrero** para entrenar y **Abril** para medir, con el fin de realizar *backtesting*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iDyeXHAuCKuT"
      },
      "outputs": [],
      "source": [
        "train_data = data[data['foto_mes'] == mes_train]\n",
        "test_data = data[data['foto_mes'] == mes_test]\n",
        "\n",
        "X_train = train_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_train_binaria1 = train_data['clase_binaria1']\n",
        "y_train_binaria2 = train_data['clase_binaria2']\n",
        "w_train = train_data['clase_peso']\n",
        "\n",
        "X_test = test_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_test_binaria1 = test_data['clase_binaria1']\n",
        "y_test_class = test_data['clase_ternaria']\n",
        "w_test = test_data['clase_peso']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scpnp1HJ6wfO"
      },
      "source": [
        "Y preparamos el *dataset* para poder usar el **rf** de una clase anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5cqbDiI4x2OD"
      },
      "outputs": [],
      "source": [
        "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "Xif = imp_mean.fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k31eSe5zlTEk"
      },
      "source": [
        "Comenzaremos explicando el funcionamiento del protagonista de esta clase: **LightGBM**. Primero, partiremos con una revisión de cómo funciona el algoritmo en el que se basa, **XGBoost**. Para una introducción completa, puedes consultar este\n",
        "\n",
        "https://xgboost.readthedocs.io/en/stable/tutorials/model.html.\n",
        "\n",
        "Aunque en la cátedra no somos grandes seguidores de Josh Starmer y su canal *StatQuest*, reconozco que sus series sobre *Gradient Boosting* y *XGBoost* son excelentes recursos. Aquí te dejamos los enlaces a esas dos series que realmente valen la pena:\n",
        "\n",
        "[Serie Gradient Boosting](https://www.youtube.com/watch?v=3CC4N4z3GJc&list=PLblh5JKOoLUJjeXUvUE0maghNuY2_5fY6)\n",
        "\n",
        "[Serie XGBoost](https://www.youtube.com/watch?v=OtD8wVaFm6E&list=PLblh5JKOoLULU0irPgs1SnKO6wqVjKUsQ)\n",
        "\n",
        "Finalmente, analizaremos las diferencias clave que ofrece **LightGBM** frente a XGBoost. Puedes explorar más sobre ello en este https://lightgbm.readthedocs.io/en/stable/Features.html.\n",
        "\n",
        "No olvides tener a mano la [documentación de LightGBM](https://lightgbm.readthedocs.io/)y la [lista completa de sus parámetros](https://lightgbm.readthedocs.io/en/latest/Parameters.html).\n",
        "\n",
        "Este es un algoritmo muy usado en el mercado, recomiendo dedicarle el tiempo necesario para aprenderlo bien."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ-3AgzL9ude"
      },
      "source": [
        "Vamos a utilizar el algoritmo directamente, sin pasar por *scikit-learn*. Sin embargo, si algún alumno lo prefiere, puede optar por usar el *wrapper* de sklearn para este caso.\n",
        "\n",
        "Para evaluar la calidad del modelo, crearemos nuestra propia función de evaluación que calcule la ganancia. La razón de incluir los pesos es precisamente para poder implementar esta función de evaluación de manera adecuada. Al combinar las clases *BAJA+1* y *BAJA+2* en una sola, necesitamos una forma de diferenciarlas, y es aquí donde entra en juego el *weight*. Este parámetro nos permitirá distinguir entre ambas clases al momento de evaluarlas dentro del algoritmo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_FvDUXeatl66"
      },
      "outputs": [],
      "source": [
        "def lgb_gan_eval(y_pred, data):\n",
        "    weight = data.get_weight()\n",
        "    ganancia = np.where(weight == 1.00002, ganancia_acierto, 0) - np.where(weight < 1.00002, costo_estimulo, 0)\n",
        "    ganancia = ganancia[np.argsort(y_pred)[::-1]]\n",
        "    ganancia = np.cumsum(ganancia)\n",
        "\n",
        "    return 'gan_eval', np.max(ganancia) , True\n",
        "\n",
        "# Parámetros del modelos.\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'gan_eval',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'max_bin': 31,\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.01,\n",
        "    'feature_fraction': 0.3,\n",
        "    'bagging_fraction': 0.7,\n",
        "    'verbose': 0\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qte_kOcU-7i3"
      },
      "source": [
        "LGBM necesita su propio tipo de Datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5dnWsRWgPnRr"
      },
      "outputs": [],
      "source": [
        "train_data1 = lgb.Dataset(X_train, label=y_train_binaria1, weight=w_train)\n",
        "train_data2 = lgb.Dataset(X_train, label=y_train_binaria2, weight=w_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GSEwNi3_IxN"
      },
      "source": [
        "A continuación, compararemos las dos clases. Utilizaremos para medir la calidad de las clases (y de los parámetros), la función **cv** que viene *out-of-the-box*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ibXn1tiLNT6J"
      },
      "outputs": [],
      "source": [
        "cv_results1 = lgb.cv(\n",
        "    params,\n",
        "    train_data1,\n",
        "    num_boost_round=150,\n",
        "    feval=lgb_gan_eval,\n",
        "    nfold=5,\n",
        "    seed=semillas[0]\n",
        ")\n",
        "\n",
        "cv_results2 = lgb.cv(\n",
        "    params,\n",
        "    train_data2,\n",
        "    num_boost_round=150,\n",
        "    feval=lgb_gan_eval,\n",
        "    nfold=5,\n",
        "    seed=semillas[0]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePRPrDL8_odf"
      },
      "source": [
        "Y vizualizamos los resultados de ambas ejecuciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "cOoWx9sbhx5h",
        "outputId": "68d27639-4af7-4003-d04c-a3a0eb327673"
      },
      "outputs": [],
      "source": [
        "df_ganancias = pd.DataFrame({\n",
        "    'binaria1': cv_results1['valid gan_eval-mean'],\n",
        "    'binaria2': cv_results2['valid gan_eval-mean'],\n",
        "    'Iteracion': range(1, len(cv_results1['valid gan_eval-mean']) + 1)\n",
        "})\n",
        "\n",
        "# Normalizamos la ganancias\n",
        "df_ganancias['binaria1'] = df_ganancias['binaria1']*5\n",
        "df_ganancias['binaria2'] = df_ganancias['binaria2']*5\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x='Iteracion', y='binaria1', data=df_ganancias, label='binaria 1')\n",
        "sns.lineplot(x='Iteracion', y='binaria2', data=df_ganancias, label='binaria 2')\n",
        "plt.title('Comparación de las Ganancias de las 2 clases binarias')\n",
        "plt.xlabel('Iteración')\n",
        "plt.ylabel('Ganancia')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N96lUJOLDqLH"
      },
      "source": [
        "Se observa una ligera mejora al combinar las clases en modelos sencillos. Dado que cada pequeña mejora es importante, continuaremos utilizando esta estrategia.\n",
        "\n",
        "A continuación, procederemos a optimizar **LightGBM** utilizando la librería **Optuna**. Cabe destacar que las optimizaciones que realizaremos son básicas y están diseñadas para ejecutarse en pocos minutos. Será su responsabilidad ampliar tanto el rango de búsqueda como el tiempo de optimización para obtener un modelo más competitivo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYMEnNFbkSoQ",
        "outputId": "15d768d4-9e25-4063-984b-d79c90ea91bc"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    \n",
        "    # Rango de parámetros a buscar sus valores óptimos.\n",
        "    num_leaves = trial.suggest_int('num_leaves', 8, 100),\n",
        "    learning_rate = trial.suggest_float('learning_rate', 0.005, 0.3), # mas bajo, más iteraciones necesita\n",
        "    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 1, 1000),\n",
        "    feature_fraction = trial.suggest_float('feature_fraction', 0.1, 1.0),\n",
        "    bagging_fraction = trial.suggest_float('bagging_fraction', 0.1, 1.0),\n",
        "\n",
        "    # Parámetros que le voy a pasar al modelo.\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'custom',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'first_metric_only': True,\n",
        "        'boost_from_average': True,\n",
        "        'feature_pre_filter': False,\n",
        "        'max_bin': 31,\n",
        "        'num_leaves': num_leaves,\n",
        "        'learning_rate': learning_rate,\n",
        "        'min_data_in_leaf': min_data_in_leaf,\n",
        "        'feature_fraction': feature_fraction,\n",
        "        'bagging_fraction': bagging_fraction,\n",
        "        'seed': semillas[0],\n",
        "        'verbose': -1\n",
        "    }\n",
        "    \n",
        "    # Creo el dataset para Light GBM.\n",
        "    train_data = lgb.Dataset(X_train,\n",
        "                              label=y_train_binaria2, # eligir la clase\n",
        "                              weight=w_train)\n",
        "    \n",
        "    # Entreno.\n",
        "    cv_results = lgb.cv(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=100, # modificar, subit y subir... y descomentar la línea inferior\n",
        "        # early_stopping_rounds= int(50 + 5 / learning_rate),\n",
        "        feval=lgb_gan_eval,\n",
        "        stratified=True,\n",
        "        nfold=5,\n",
        "        seed=semillas[0]\n",
        "    )\n",
        "    \n",
        "    # Calculo la ganancia máxima y la mejor iteración donde se obtuvo dicha ganancia.\n",
        "    max_gan = max(cv_results['valid gan_eval-mean'])\n",
        "    best_iter = cv_results['valid gan_eval-mean'].index(max_gan) + 1\n",
        "\n",
        "    # Guardamos cual es la mejor iteración del modelo\n",
        "    trial.set_user_attr(\"best_iter\", best_iter)\n",
        "\n",
        "    return max_gan * 5\n",
        "\n",
        "\n",
        "# Guardo el estudio de Optuna en una BBDD.\n",
        "storage_name = \"sqlite:///\" + db_path + \"optimization_lgbm.db\"\n",
        "study_name = \"exp_301_lgbm\"\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=study_name,\n",
        "    storage=storage_name,\n",
        "    load_if_exists=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMrT22K0u9JF",
        "outputId": "75ee1c52-fb2c-4ad4-df1e-1d0852a3867e"
      },
      "outputs": [],
      "source": [
        "# study.optimize(objective, n_trials=100) # subir subir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia2vN07FEasX"
      },
      "source": [
        "Analizamos los resultados as usual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "fH4ybQgYx7Xf",
        "outputId": "e2492e33-7da9-408e-bc68-ecc14c0fdf99"
      },
      "outputs": [],
      "source": [
        "optuna.visualization.plot_optimization_history(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "vOfm5DXAx8Rj",
        "outputId": "7ea05841-331e-48cb-fcb9-c36e97ab2051"
      },
      "outputs": [],
      "source": [
        "plot_param_importances(study)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0Z-r8QYEsNN"
      },
      "source": [
        "El **learning rate** es un parámetro que tiene que ir acompañado por más árboles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "0U6CfznSx-gG",
        "outputId": "f3c4a028-2d47-4e6e-ba8b-2877fe9d843a"
      },
      "outputs": [],
      "source": [
        "plot_slice(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "XRqPgCD6yB_q",
        "outputId": "0adae389-2981-43ee-c939-212232dd4f31"
      },
      "outputs": [],
      "source": [
        "plot_contour(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "wGHGdGcQ3m00",
        "outputId": "624ee6d0-a5f1-4b00-bac6-8f37de7e99a7"
      },
      "outputs": [],
      "source": [
        "plot_contour(study, params=['num_leaves','min_data_in_leaf'] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjgD6raVE6am"
      },
      "source": [
        "Y finalmente tomamos el mejor modelo y lo entrenamos con la totalidad de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwyUriQksZAM",
        "outputId": "c4b54b3e-75b0-4076-8a31-7096bd94382b"
      },
      "outputs": [],
      "source": [
        "best_iter = study.best_trial.user_attrs[\"best_iter\"]\n",
        "print(f\"Mejor cantidad de árboles para el mejor model {best_iter}\")\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'first_metric_only': True,\n",
        "    'boost_from_average': True,\n",
        "    'feature_pre_filter': False,\n",
        "    'max_bin': 31,\n",
        "    'num_leaves': study.best_trial.params['num_leaves'],\n",
        "    'learning_rate': study.best_trial.params['learning_rate'],\n",
        "    'min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],\n",
        "    'feature_fraction': study.best_trial.params['feature_fraction'],\n",
        "    'bagging_fraction': study.best_trial.params['bagging_fraction'],\n",
        "    'seed': semillas[0],\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "train_data = lgb.Dataset(X_train,\n",
        "                          label=y_train_binaria2,\n",
        "                          weight=w_train)\n",
        "\n",
        "model = lgb.train(params,\n",
        "                  train_data,\n",
        "                  num_boost_round=best_iter)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOyqa5mbFySM"
      },
      "source": [
        "Observamos la variables más importantes para el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xUejb7eutd0i",
        "outputId": "bdcd9641-679e-4de5-d0f8-cffdfc3bf241"
      },
      "outputs": [],
      "source": [
        "lgb.plot_importance(model, figsize=(10, 20))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkTH9daXF5tp"
      },
      "source": [
        "Y si queremos tener las variables más importantes en forma de *Dataframe*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "l7ZObpkHtnUl",
        "outputId": "eb0fe3a2-9e41-4886-ebd7-257dd1cf9829"
      },
      "outputs": [],
      "source": [
        "importances = model.feature_importance()\n",
        "feature_names = X_train.columns.tolist()\n",
        "importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "importance_df = importance_df.sort_values('importance', ascending=False)\n",
        "importance_df[importance_df['importance'] > 0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwvqxqc_GB-C"
      },
      "source": [
        "Para guardar el modelo para poder utilizarlo más adelante, no es necesario guardarlo como *pickle*, la librería nos permite guardarlo en formato texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0lWWxwHhs2gp"
      },
      "outputs": [],
      "source": [
        "# model.save_model(modelos_path + 'lgb_first.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHIUeAdsGOWv"
      },
      "source": [
        "Y recuperar el mismo desde ese formato"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "H12h5wP2s645"
      },
      "outputs": [],
      "source": [
        "model = lgb.Booster(model_file=modelos_path + 'lgb_first.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_-xspdWGSwT"
      },
      "source": [
        "Para realizar nuestra habitual comparación de modelos, partiremos desde el mejor que obtuvimos hasta ahora, el **rf**. Para este fin cargaremos el *binario* que ajustamos un par de clases atrás:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "J31X1oeRwxbt"
      },
      "outputs": [],
      "source": [
        "filename_rf_1000 = modelos_path + 'exp_206_random_forest_model_1000.sav'\n",
        "model_rf_1000 = pickle.load(open(filename_rf_1000, 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28TQpPlIGi6a"
      },
      "source": [
        "Y sobre ambos modelos obtenemos la predicción de **Abril**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "kL9dBAv4xWz2"
      },
      "outputs": [],
      "source": [
        "y_pred_rf = model_rf_1000.predict_proba(Xif)\n",
        "y_pred_rf = y_pred_rf[:,1] # adaptamos la salida para que sea homogénea con el LGBM\n",
        "\n",
        "y_pred_lgm = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtivqQ2OGvxC"
      },
      "source": [
        "Finalmente medimos las ganancias de ambos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSvtslh-uMcK",
        "outputId": "27b8ea9d-cc86-4d58-af19-71d78772cbc3"
      },
      "outputs": [],
      "source": [
        "def ganancia_prob(y_pred, y_true, prop = 1):\n",
        "  ganancia = np.where(y_true == 1, ganancia_acierto, 0) - np.where(y_true == 0, costo_estimulo, 0)\n",
        "  return ganancia[y_pred >= 0.025].sum() / prop\n",
        "\n",
        "print(\"Ganancia RF:\", ganancia_prob(y_pred_rf, y_test_binaria1))\n",
        "print(\"Ganancia LGBM:\", ganancia_prob(y_pred_lgm, y_test_binaria1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KdAKkYQGz7K"
      },
      "source": [
        "Vemos un nuevo salto, tan alto como el del árbol al rf. Será simplemente suerte? veamos que sucede sobre múltiples **LDB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "q-EFxmQtzcs0"
      },
      "outputs": [],
      "source": [
        "sss_futuro = StratifiedShuffleSplit(n_splits=50,\n",
        "                             test_size=0.3,\n",
        "                             random_state=semillas[0])\n",
        "modelos = {\"rf\":y_pred_rf, \"lgbm\":y_pred_lgm}\n",
        "rows = []\n",
        "for private_index, public_index in sss_futuro.split(X_test, y_test_binaria1):\n",
        "  row = {}\n",
        "  for name, y_pred in modelos.items():\n",
        "    row[name + \"_public\"] = ganancia_prob(y_pred[public_index], y_test_binaria1.iloc[public_index], 0.3)\n",
        "    row[name + \"_private\"] = ganancia_prob(y_pred[private_index], y_test_binaria1.iloc[private_index], 0.7)\n",
        "  rows.append(row)\n",
        "df_lb = pd.DataFrame(rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "76Adb5Q6zhG7"
      },
      "outputs": [],
      "source": [
        "df_lb_long = df_lb.reset_index()\n",
        "df_lb_long = df_lb_long.melt(id_vars=['index'], var_name='model_type', value_name='ganancia')\n",
        "df_lb_long[['modelo', 'tipo']] = df_lb_long['model_type'].str.split('_', expand=True)\n",
        "df_lb_long = df_lb_long[['ganancia', 'tipo', 'modelo']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "ZTG3w3UXzjQK",
        "outputId": "4b616175-e969-4d8e-d5cb-17f3ebd7e8ae"
      },
      "outputs": [],
      "source": [
        "g = sns.FacetGrid(df_lb_long, col=\"tipo\", row=\"modelo\", aspect=2)\n",
        "g.map(sns.histplot, \"ganancia\", kde=True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsPshEZVHTw_"
      },
      "source": [
        "Otra vez se observa la superioridad del **LGBM**. Veamos que hubiera pasado si elegíamos el mejor del público"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "cQ9CARjWzeIm",
        "outputId": "4b3b6229-e9e2-4f2d-9bbe-3b1b42feb02a"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame()\n",
        "df['best_public'] = df_lb.filter(regex='_public').idxmax(axis=1)\n",
        "df['best_private'] = df_lb.filter(regex='_private').idxmax(axis=1)\n",
        "\n",
        "pd.crosstab(df['best_public'], df['best_private'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7rwLi7IHi3-"
      },
      "source": [
        "Observamos que en 50 **LDBs** solo en un público ganó un **rf**, sin embargo en el 100% de los casos, en el privado ganó un **lgbm**. Asombroso.\n",
        "\n",
        "Pero el alumno atento, vio que para la selección del mejor modelo no se utilizó ningún punto de corte. Podrá pasar que el mejor punto de corte en entrenamiento, no sea el mejor para un mes en el futuro?\n",
        "\n",
        "Veamos para esto la curva de ganancia en función de los puntos de corte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "85csWJ6p5huj"
      },
      "outputs": [],
      "source": [
        "ganancia = np.where(y_test_binaria1 == 1, ganancia_acierto, 0) - np.where(y_test_binaria1 == 0, costo_estimulo, 0)\n",
        "\n",
        "idx = np.argsort(y_pred_lgm)[::-1]\n",
        "\n",
        "ganancia = ganancia[idx]\n",
        "y_pred_lgm = y_pred_lgm[idx]\n",
        "\n",
        "ganancia_cum = np.cumsum(ganancia)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "yULvAtz964Ek",
        "outputId": "0abfbb79-2f4c-4a53-cc5e-7c6cc947e969"
      },
      "outputs": [],
      "source": [
        "piso_envios = 4000\n",
        "techo_envios = 20000\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_pred_lgm[piso_envios:techo_envios], ganancia_cum[piso_envios:techo_envios], label='Ganancia LGBM')\n",
        "plt.title('Curva de Ganancia')\n",
        "plt.xlabel('Predicción de probabilidad')\n",
        "plt.ylabel('Ganancia')\n",
        "plt.axvline(x=0.025, color='g', linestyle='--', label='Punto de corte a 0.025')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrRvBfUEIM0Y"
      },
      "source": [
        "Vaya! realmente nuestro teórico mejor punto de corte no es el que mayor ganancia genera. Es hora de cambiar el enfoque.\n",
        "\n",
        "En vez de mirar el punto de corte, empezaremos a pensar en cuál es la cantidad máxima de clientes que se deben estimular. Si cambiamos a esto, veremos que el gráfico anterior se ve así"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "LgLC9YI15xr8",
        "outputId": "b403cf68-5cd1-40aa-8653-03cc3a73dcf0"
      },
      "outputs": [],
      "source": [
        "piso_envios = 4000\n",
        "techo_envios = 20000\n",
        "\n",
        "ganancia_max = ganancia_cum.max()\n",
        "gan_max_idx = np.where(ganancia_cum == ganancia_max)[0][0]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(piso_envios, len(ganancia_cum[piso_envios:techo_envios]) + piso_envios), ganancia_cum[piso_envios:techo_envios], label='Ganancia LGBM')\n",
        "plt.axvline(x=gan_max_idx, color='g', linestyle='--', label=f'Punto de corte a la ganancia máxima {gan_max_idx}')\n",
        "plt.axhline(y=ganancia_max, color='r', linestyle='--', label=f'Ganancia máxima {ganancia_max}')\n",
        "plt.title('Curva de Ganancia')\n",
        "plt.xlabel('Clientes')\n",
        "plt.ylabel('Ganancia')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAxBMAYzI2yW"
      },
      "source": [
        "Lo que significa que la cantidad de envíos que maximiza la ganancia es 12601.\n",
        "\n",
        "Claro, estamos haciendo trampa. No nunca vamos a contar con datos del futuro para determinar este punto de corte...  o sí?\n",
        "\n",
        "En nuestro caso, si contamos con una pequeña ventana de datos del futuro, el **leaderboard público**.\n",
        "\n",
        "Realicemos un análisis para determinar si el leaderboard puede ayudarnos a identificar el punto de corte óptimo que maximice la ganancia en el conjunto de datos privado.\n",
        "\n",
        "En la siguiente función cambie la semilla para evaluar como cambia los **leaderboards**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "MDSE3m2c716w",
        "outputId": "f99f0e47-9ddb-46ae-ba2b-ca75022ef3fc"
      },
      "outputs": [],
      "source": [
        "def analisis_1(semilla):\n",
        "  df_cut_point = pd.DataFrame({'ganancia': ganancia, 'y_pred_lgm': y_pred_lgm})\n",
        "\n",
        "  private_idx, public_idx = train_test_split(df_cut_point.index, test_size=0.3, random_state=semilla, stratify=y_test_binaria1)\n",
        "\n",
        "  df_cut_point['public'] = 0.0\n",
        "  df_cut_point['private'] = 0.0\n",
        "  df_cut_point.loc[private_idx, 'private'] = ganancia[private_idx] / 0.7\n",
        "  df_cut_point.loc[public_idx, 'public'] = ganancia[public_idx] / 0.3\n",
        "\n",
        "  df_cut_point['nro_envios'] = df_cut_point.reset_index().index\n",
        "\n",
        "  df_cut_point['public_cum'] = df_cut_point['public'].cumsum()\n",
        "  df_cut_point['private_cum'] = df_cut_point['private'].cumsum()\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(df_cut_point['nro_envios'][4000:20000], df_cut_point['public_cum'][4000:20000], label='Ganancia Pública Acumulada')\n",
        "  plt.plot(df_cut_point['nro_envios'][4000:20000], df_cut_point['private_cum'][4000:20000], label='Ganancia Privada Acumulada')\n",
        "\n",
        "  max_public_cum = df_cut_point['public_cum'][4000:20000].max()\n",
        "  max_public_idx = df_cut_point['public_cum'][4000:20000].idxmax()\n",
        "  plt.axvline(x=max_public_idx, color='g', linestyle='--', label=f'Máximo Ganancia Pública en {max_public_idx}')\n",
        "\n",
        "  max_private_cum = df_cut_point['private_cum'][4000:20000].max()\n",
        "  max_private_idx = df_cut_point['private_cum'][4000:20000].idxmax()\n",
        "  plt.axvline(x=max_private_idx, color='r', linestyle='--', label=f'Máximo Ganancia Privada en {max_private_idx}')\n",
        "\n",
        "  plt.title('Curva de Ganancia Pública y Privada')\n",
        "  plt.xlabel('Número de envíos')\n",
        "  plt.ylabel('Ganancia Acumulada')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "analisis_1(semillas[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Q-wD6tKYUm"
      },
      "source": [
        "Qué conclusión sacó?\n",
        "\n",
        "Hay un nuevo pero. Sería fabuloso poder contar con la curva de todo el **leaderboard público**, pero no es posible. Hay una cantidad finita de envíos. El siguiente análisis de back testing es más exacto.\n",
        "\n",
        "**JUEGUE**\n",
        "- Ponga en False el parámetro **private**\n",
        "- Cambie la semilla\n",
        "- Determine la cantidad de casos que va a enviar al leaderboard público. Recuerde que no tiene más de 20 por día.\n",
        "- Determine la cantidad optima mirando la gráfica\n",
        "- Ponga el True poder visualizar la curva del **leaderboard privado**\n",
        "- Evalue que tan bien le acerto.\n",
        "- Repita"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "pcWJmHPlchV5",
        "outputId": "8acc8fd1-8ede-45d5-e39e-1e810bdbd1e4"
      },
      "outputs": [],
      "source": [
        "def analisis_2(semilla, desde, paso, cantidad, private = False):\n",
        "\n",
        "  df_cut_point = pd.DataFrame({'ganancia': ganancia, 'y_pred_lgm': y_pred_lgm})\n",
        "  df_cut_point['nro_envios'] = df_cut_point.reset_index().index\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "\n",
        "  private_idx, public_idx = train_test_split(df_cut_point.index, test_size=0.3, random_state=semilla, stratify=y_test_binaria1)\n",
        "\n",
        "  df_cut_point['public'] = 0.0\n",
        "  df_cut_point.loc[public_idx, 'public'] = ganancia[public_idx] / 0.3\n",
        "  df_cut_point['public_cum'] = df_cut_point['public'].cumsum()\n",
        "\n",
        "  maximo_paso = desde + paso*cantidad\n",
        "  plt.plot(df_cut_point['nro_envios'][list(range(desde, maximo_paso + 1, paso))], df_cut_point['public_cum'][list(range(desde, maximo_paso + 1, paso))], label='Ganancia Pública Acumulada')\n",
        "  max_public_cum = df_cut_point['public_cum'][list(range(desde, maximo_paso + 1, paso))].max()\n",
        "  max_public_idx = df_cut_point['public_cum'][list(range(desde, maximo_paso + 1, paso))].idxmax()\n",
        "  plt.axvline(x=max_public_idx, color='g', linestyle='--', label=f'Máximo Ganancia Pública en {max_public_idx}')\n",
        "\n",
        "  if private:\n",
        "    df_cut_point['private'] = 0.0\n",
        "    df_cut_point.loc[private_idx, 'private'] = ganancia[private_idx] / 0.7\n",
        "    df_cut_point['private_cum'] = df_cut_point['private'].cumsum()\n",
        "    plt.plot(df_cut_point['nro_envios'][4000:20000], df_cut_point['private_cum'][4000:20000], label='Ganancia Privada Acumulada')\n",
        "    max_private_cum = df_cut_point['private_cum'][4000:20000].max()\n",
        "    max_private_idx = df_cut_point['private_cum'][4000:20000].idxmax()\n",
        "    plt.axvline(x=max_private_idx, color='r', linestyle='--', label=f'Máximo Ganancia Privada en {max_private_idx}')\n",
        "\n",
        "  plt.title('Curva de Ganancia Pública y Privada')\n",
        "  plt.xlabel('Número de envíos')\n",
        "  plt.ylabel('Ganancia Acumulada')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "analisis_2(semillas[1], 4000, 500, 10, private=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Aat38kYztkq"
      },
      "source": [
        "# Tarea:\n",
        "\n",
        "1. **Generar Dataset**  \n",
        "   - Utilice las técnicas de *feature engineering* vistas en las clases anteriores para generar un nuevo conjunto de datos.\n",
        "   \n",
        "2. **Optimización de LightGBM (LGBM)**  \n",
        "   - Ajuste el modelo de LightGBM utilizando una mayor cantidad de árboles y realice una exploración más exhaustiva de los hiperparámetros para mejorar su rendimiento.\n",
        "   \n",
        "3. **Incluir Nuevos Parámetros en la Optimización**  \n",
        "   - Revise la documentación de los parámetros de LightGBM. Evalúe la inclusión de otros parámetros en el proceso de optimización, y ajuste el modelo con estos nuevos parámetros.\n",
        "   \n",
        "4. **Selección del Mejor Modelo**  \n",
        "   - Entre los cinco mejores modelos obtenidos en cada optimización, seleccione el que considere más adecuado para la competencia en Kaggle.\n",
        "   - Documente las pruebas que realizó para seleccionar el mejor modelo. Justifique su decisión con métricas relevantes y análisis comparativos.\n",
        "5. Escriba y comparta por **Zulip** una función que envíe prepare el dataset que es necesario enviar a **kaggle** con los N clientes con mayor probabilidad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Preparación previa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#A. Librerías.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "import optuna\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_slice, plot_contour\n",
        "\n",
        "from time import time\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#B. Constantes.\n",
        "base_path = 'C:/Users/c678456/Desktop/Ian/Maestría/Especializacion/2do_cuatrimestre/DMEyF/'\n",
        "dataset_file_crudo = 'datasets/competencia_01.csv'\n",
        "dataset_file_fe = 'datasets/competencia_01_fe.csv'\n",
        "modelos_path = base_path + 'modelos/'\n",
        "db_path = base_path + 'db/'\n",
        "\n",
        "ganancia_acierto = 273000\n",
        "costo_estimulo = 7000\n",
        "\n",
        "mes_train = [202101,202102,202103,202104]\n",
        "mes_test = 202106\n",
        "\n",
        "# agregue sus semillas\n",
        "semillas = [211777, 174989, 131497, 612223, 234803]\n",
        "\n",
        "data = pd.read_csv(base_path + dataset_file_crudo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#C. Funciones.\n",
        "def lgb_gan_eval(y_pred, data):\n",
        "    weight = data.get_weight()\n",
        "    ganancia = np.where(weight == 1.00002, ganancia_acierto, 0) - np.where(weight < 1.00002, costo_estimulo, 0)\n",
        "    ganancia = ganancia[np.argsort(y_pred)[::-1]]\n",
        "    ganancia = np.cumsum(ganancia)\n",
        "\n",
        "    return 'gan_eval', np.max(ganancia) , True\n",
        "\n",
        "def objective(trial): \n",
        "    # Rango de parámetros a buscar sus valores óptimos.\n",
        "    num_leaves = trial.suggest_int('num_leaves', 50, 500)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 0.005, 0.3) # mas bajo, más iteraciones necesita\n",
        "    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 500, 4000)\n",
        "    feature_fraction = trial.suggest_float('feature_fraction', 0.5, 1.0)\n",
        "    bagging_fraction = trial.suggest_float('bagging_fraction', 0.4, 1.0)\n",
        "    max_depth = trial.suggest_int('max_depth', 3, 15) # Nuevo parámetro.\n",
        "    lambda_l2 = trial.suggest_float('lambda_l2', 1e-8, 10.0) # Nuevo parámetro.\n",
        "\n",
        "\n",
        "    # Parámetros que le voy a pasar al modelo.\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'custom',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'first_metric_only': True,\n",
        "        'boost_from_average': True,\n",
        "        'feature_pre_filter': False,\n",
        "        'max_bin': 31,\n",
        "        'num_leaves': num_leaves,\n",
        "        'learning_rate': learning_rate,\n",
        "        'min_data_in_leaf': min_data_in_leaf,\n",
        "        'feature_fraction': feature_fraction,\n",
        "        'bagging_fraction': bagging_fraction,\n",
        "        'max_depth': max_depth,\n",
        "        'lambda_l2': lambda_l2,\n",
        "        'seed': semillas[0],\n",
        "        'verbose': -1\n",
        "    }\n",
        "    \n",
        "    # Creo el dataset para Light GBM.\n",
        "    train_data = lgb.Dataset(X_train,\n",
        "                              label=y_train_binaria2, # eligir la clase\n",
        "                              weight=w_train)\n",
        "    \n",
        "    # Entreno.\n",
        "    cv_results = lgb.cv(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=1000, # modificar, subit y subir... y descomentar la línea inferior\n",
        "        callbacks=[lgb.early_stopping(int(50 + 5 / learning_rate))],\n",
        "        feval=lgb_gan_eval,\n",
        "        stratified=True,\n",
        "        nfold=5,\n",
        "        seed=semillas[0]\n",
        "    )\n",
        "    \n",
        "    # Calculo la ganancia máxima y la mejor iteración donde se obtuvo dicha ganancia.\n",
        "    max_gan = max(cv_results['valid gan_eval-mean'])\n",
        "    best_iter = cv_results['valid gan_eval-mean'].index(max_gan) + 1\n",
        "\n",
        "    # Guardamos cual es la mejor iteración del modelo\n",
        "    trial.set_user_attr(\"best_iter\", best_iter)\n",
        "\n",
        "    return max_gan * 5\n",
        "\n",
        "def ganancia_prob(y_pred, y_true, prop = 1):\n",
        "  ganancia = np.where(y_true == 1, ganancia_acierto, 0) - np.where(y_true == 0, costo_estimulo, 0)\n",
        "  return ganancia[y_pred >= 0.025].sum() / prop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#D. Pesos y reclusterización.\n",
        "data['clase_peso'] = 1.0\n",
        "\n",
        "data.loc[data['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
        "data.loc[data['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001\n",
        "\n",
        "data['clase_binaria1'] = 0\n",
        "data['clase_binaria2'] = 0\n",
        "data['clase_binaria1'] = np.where(data['clase_ternaria'] == 'BAJA+2', 1, 0)\n",
        "data['clase_binaria2'] = np.where(data['clase_ternaria'] == 'CONTINUA', 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#E. Divido entre Train y Test.\n",
        "train_data = data[data['foto_mes'].isin(mes_train)]\n",
        "test_data = data[data['foto_mes'] == mes_test]\n",
        "\n",
        "X_train = train_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_train_binaria1 = train_data['clase_binaria1']\n",
        "y_train_binaria2 = train_data['clase_binaria2']\n",
        "w_train = train_data['clase_peso']\n",
        "\n",
        "X_test = test_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_test_binaria1 = test_data['clase_binaria1']\n",
        "y_test_class = test_data['clase_ternaria']\n",
        "w_test = test_data['clase_peso']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Predicción con datos crudos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#a. Voy a realizar un estudio de Optuna para encontrar los mejores parámetros.\n",
        "#i. Creo la base de datos donde guardar los resultados.\n",
        "storage_name = \"sqlite:///\" + db_path + \"optimization_lgbm.db\"\n",
        "study_name = \"exp_300_lgbm_datos_crudos_100_num_boost_round\"\n",
        "\n",
        "#ii. Creo el estudio.\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=study_name,\n",
        "    storage=storage_name,\n",
        "    load_if_exists=True,\n",
        ")\n",
        "\n",
        "#iii. Corro el estudio.\n",
        "study.optimize(objective, n_trials=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#b. Visualizo los resultados del estudio, para modificar los rangos de análisis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optuna.visualization.plot_optimization_history(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_param_importances(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_slice(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Podría aumentar el bagging fraction y el num_leaves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_contour(study, params=['num_leaves','min_data_in_leaf'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#c. Tomamos el mejor modelo y con eso entrenamos todos los datos.\n",
        "best_iter = study.best_trial.user_attrs[\"best_iter\"]\n",
        "print(f\"Mejor cantidad de árboles para el mejor model {best_iter}\")\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'first_metric_only': True,\n",
        "    'boost_from_average': True,\n",
        "    'feature_pre_filter': False,\n",
        "    'max_bin': 31,\n",
        "    'num_leaves': study.best_trial.params['num_leaves'],\n",
        "    'learning_rate': study.best_trial.params['learning_rate'],\n",
        "    'min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],\n",
        "    'feature_fraction': study.best_trial.params['feature_fraction'],\n",
        "    'bagging_fraction': study.best_trial.params['bagging_fraction'],\n",
        "    'seed': semillas[0],\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "train_data = lgb.Dataset(X_train,\n",
        "                          label=y_train_binaria2,\n",
        "                          weight=w_train)\n",
        "\n",
        "model_lgb = lgb.train(params,\n",
        "                  train_data,\n",
        "                  num_boost_round=best_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#d. Observamos las variables más importantes para el modelo.\n",
        "lgb.plot_importance(model_lgb, figsize=(10, 20))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#e. Guardamos el modelo.\n",
        "model_lgb.save_model('lgb_datos_crudos_01.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#f. Volvemos a leer el modelo.\n",
        "model_lgb = lgb.Booster(model_file=modelos_path + 'lgb_datos_crudos_01.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "#g. Predecimos Junio.\n",
        "#i. Realizo la predicción de probabilidades usando el modelo entrenado.\n",
        "predicciones = model_lgb.predict(X_test)\n",
        "#ii. Convierto las probabilidades a clases usando el punto de corte 0.025.\n",
        "clases = [1 if prob >= 0.025 else 0 for prob in predicciones]\n",
        "#iii. Solo envío estímulo a los registros con probabilidad de \"BAJA+2\" mayor a 1/40.\n",
        "X_test['Predicted'] = clases\n",
        "#v. Selecciono las columnas de interés.\n",
        "resultados = X_test[[\"numero_de_cliente\", 'Predicted']].reset_index(drop=True)\n",
        "#vi. Exporto como archivo .csv.\n",
        "nombre_archivo = \"K106_001.csv\"\n",
        "ruta_archivo= \"../../../exp/{}\".format(nombre_archivo)\n",
        "resultados.to_csv(ruta_archivo, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#h. Envío a Kaggle.\n",
        "#a. Importo librería.\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "#b. Configura el API de Kaggle\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "#c. Defino los parámetros claves.\n",
        "mensaje = f'Archivo {nombre_archivo}. LGB de datos crudos. No se imputa, 100 Trials para búsqueda de hiperparámetros, 100 boost.'\n",
        "competencia = 'dm-ey-f-2024-primera'\n",
        "#c. Subo la Submission.\n",
        "api.competition_submit(file_name=ruta_archivo,message=mensaje,competition=competencia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset con feature engineering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#a. Preparación previa del dataset.\n",
        "#i. Leo el Dataset.\n",
        "data_fe = pd.read_csv(base_path + dataset_file_fe)\n",
        "\n",
        "#ii. Asigno pesos.\n",
        "data_fe['clase_peso'] = 1.0\n",
        "\n",
        "data_fe.loc[data_fe['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
        "data_fe.loc[data_fe['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001\n",
        "\n",
        "data_fe['clase_binaria1'] = 0\n",
        "data_fe['clase_binaria2'] = 0.\n",
        "data_fe['clase_binaria1'] = np.where(data_fe['clase_ternaria'] == 'BAJA+2', 1, 0)\n",
        "data_fe['clase_binaria2'] = np.where(data_fe['clase_ternaria'] == 'CONTINUA', 0, 1)\n",
        "\n",
        "#iii. Divido entre train y test.\n",
        "train_data = data_fe[data_fe['foto_mes'].isin(mes_train)]\n",
        "test_data = data_fe[data_fe['foto_mes'] == mes_test]\n",
        "\n",
        "X_train = train_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_train_binaria1 = train_data['clase_binaria1']\n",
        "y_train_binaria2 = train_data['clase_binaria2']\n",
        "w_train = train_data['clase_peso']\n",
        "\n",
        "X_test = test_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_test_binaria1 = test_data['clase_binaria1']\n",
        "y_test_class = test_data['clase_ternaria']\n",
        "w_test = test_data['clase_peso']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#b. Voy a realizar un estudio de Optuna para encontrar los mejores parámetros.\n",
        "#i. Creo la base de datos donde guardar los resultados.\n",
        "storage_name = \"sqlite:///\" + db_path + \"optimization_lgbm.db\"\n",
        "study_name = \"exp_303_lgbm_datos_fe_100_num_boost_round_ajusto_limites_hiperparametros\"\n",
        "\n",
        "#ii. Creo el estudio.\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=study_name,\n",
        "    storage=storage_name,\n",
        "    load_if_exists=True,\n",
        ")\n",
        "\n",
        "#iii. Corro el estudio.\n",
        "study.optimize(objective, n_trials=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "#c. Visualizo los resultados del estudio, para modificar los rangos de análisis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optuna.visualization.plot_optimization_history(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_param_importances(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_slice(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debería hacer los limites de bagging_fraction entre: 0.4 y 0.8\n",
        "# feature fraction: 0.5 y 1\n",
        "# learning_rate: 0.1 y 0.2.\n",
        "# min_data_in_leaf: entre 750 y más de 1000\n",
        "# num_leaves: entre 50 y más de 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_contour(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_contour(study, params=['num_leaves','min_data_in_leaf'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "study.best_trial.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#d. Tomamos el mejor modelo y con eso entrenamos todos los datos.\n",
        "best_iter = study.best_trial.user_attrs[\"best_iter\"]\n",
        "print(f\"Mejor cantidad de árboles para el mejor model {best_iter}\")\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'first_metric_only': True,\n",
        "    'boost_from_average': True,\n",
        "    'feature_pre_filter': False,\n",
        "    'max_bin': 31,\n",
        "    'num_leaves': study.best_trial.params['num_leaves'],\n",
        "    'learning_rate': study.best_trial.params['learning_rate'],\n",
        "    'min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],\n",
        "    'feature_fraction': study.best_trial.params['feature_fraction'],\n",
        "    'bagging_fraction': study.best_trial.params['bagging_fraction'],\n",
        "    'num_leaves': study.best_trial.params['num_leaves'],\n",
        "    'max_depth': study.best_trial.params['max_depth'],\n",
        "    'lambda_l2': study.best_trial.params['lambda_l2'],\n",
        "    'seed': semillas[0],\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "train_data = lgb.Dataset(X_train,\n",
        "                          label=y_train_binaria2,\n",
        "                          weight=w_train)\n",
        "\n",
        "model_lgb = lgb.train(params,\n",
        "                  train_data,\n",
        "                  num_boost_round=best_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#e. Observamos las variables más importantes para el modelo.\n",
        "lgb.plot_importance(model_lgb, figsize=(10, 20))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#f. Guardamos el modelo.\n",
        "model_lgb.save_model('./lgb_datos_fe_02_ajusto_optuna.txt')\n",
        "#model_lgb.save_model(modelos_path + 'lgb_datos_fe_02_ajusto_optuna.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#g. Volvemos a leer el modelo.\n",
        "model_lgb = lgb.Booster('./lgb_datos_fe_02_ajusto_optuna.txt')\n",
        "#model_lgb = lgb.Booster(model_file=modelos_path + 'lgb_datos_fe_02_ajusto_optuna.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#h. Predecimos Junio.\n",
        "#i. Predecimos propiamente dicho.\n",
        "predicciones = model_lgb.predict(X_test)\n",
        "#ii. Le pegamos la probabilidad de ser \"BAJA\" a cada cliente.\n",
        "X_test['Probabilidad'] = predicciones\n",
        "#iii. Ordenamos a los clientes por probabilidad de ser \"BAJA\" de forma descendente.\n",
        "tb_entrega = X_test.sort_values(by='Probabilidad', ascending=False)\n",
        "#iv. Genero una lista de distintos cortes candidatos, para enviar a Kaggle.\n",
        "cortes = range(9000,14000,100)\n",
        "#v. Generamos las distintas predicciones de clases a partir de los distintos cortes posibles.\n",
        "num_subida_kaggle = 67\n",
        "for envios in cortes:\n",
        "    #1. Le ponemos clase 1 (\"BAJA\") a los primeros \"envios\" con mayor probabilidad.\n",
        "    tb_entrega['Predicted'] = 0\n",
        "    tb_entrega.iloc[:envios, tb_entrega.columns.get_loc('Predicted')] = 1\n",
        "    resultados = tb_entrega[[\"numero_de_cliente\", 'Predicted']].reset_index(drop=True)\n",
        "    \n",
        "    print(\"Cantidad de clientes {}\".format(envios))\n",
        "    #2. Guardamos el archivo para Kaggle.\n",
        "    nombre_archivo = \"K106_00{}.csv\".format(num_subida_kaggle)\n",
        "    ruta_archivo= \"../../../exp/{}\".format(nombre_archivo)\n",
        "    resultados.to_csv(ruta_archivo, index=False)\n",
        "    \n",
        "    num_subida_kaggle += 1\n",
        "    \n",
        "    #3. Envío a Kaggle.\n",
        "    #a. Defino los parámetros claves.\n",
        "    mensaje = f'Archivo {nombre_archivo}. LGB Optuna optimizado con FE (lgb_datos_fe_02_ajusto_optuna), punto_corte: {envios}. No se imputa, 100 Trials para búsqueda de hiperparámetros, 1000 boost con stop early'\n",
        "    competencia = 'dm-ey-f-2024-primera'\n",
        "    #c. Subo la Submission.\n",
        "    api.competition_submit(file_name=ruta_archivo,message=mensaje,competition=competencia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#h. Predecimos Junio (con distintos puntos de corte de probabilidades de ser \"BAJA\").\n",
        "#num_subida_kaggle = 2\n",
        "#for punto_corte in np.arange(0.045, 0.046, 0.001):\n",
        "    #i. Realizo la predicción de probabilidades usando el modelo entrenado.\n",
        "#    predicciones = model_lgb.predict(X_test)\n",
        "    #ii. Convierto las probabilidades a clases usando el punto de corte.\n",
        "#    clases = [1 if prob >= punto_corte else 0 for prob in predicciones]\n",
        "    #iii. Solo envío estímulo a los registros con probabilidad de \"BAJA+2\" mayor al punto de corte.\n",
        "#    X_test['Predicted'] = clases\n",
        "    #iv. Selecciono las columnas de interés.\n",
        "#    resultados = X_test[[\"numero_de_cliente\", 'Predicted']].reset_index(drop=True)\n",
        "    #v. Exporto como archivo .csv.\n",
        "#    nombre_archivo = \"K106_00{}.csv\".format(num_subida_kaggle)\n",
        "#    ruta_archivo= \"../../../exp/{}\".format(nombre_archivo)\n",
        "#    resultados.to_csv(ruta_archivo, index=False)\n",
        "    \n",
        "#    num_subida_kaggle += 1\n",
        "#    del X_test['Predicted']\n",
        "    \n",
        "    #i. Envío a Kaggle.\n",
        "    #a. Defino los parámetros claves.\n",
        "#    mensaje = f'Archivo {nombre_archivo}. LGB con FE punto_corte: {punto_corte}. No se imputa, 100 Trials para búsqueda de hiperparámetros, 100 boost.'\n",
        "#    competencia = 'dm-ey-f-2024-primera'\n",
        "    #c. Subo la Submission.\n",
        "#    api.competition_submit(file_name=ruta_archivo,message=mensaje,competition=competencia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Pruebo LGBM pero con distintos cortes temporales:\n",
        "- A) Solo abril (Lunes).\n",
        "- B) Abril con lag de -2 (Martes). \n",
        "- C) Febrero, Marzo y Abril con lag de -1 (Miércoles).\n",
        "- Luego, intentar agregar proporción/promedio y deltas -1 (Jueves).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### A. Solo abril (drifting con -3 meses)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#A. Librerías.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "import optuna\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_slice, plot_contour\n",
        "\n",
        "from time import time\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#B. Constantes.\n",
        "base_path = 'C:/Users/c678456/Desktop/Ian/Maestría/Especializacion/2do_cuatrimestre/DMEyF/'\n",
        "dataset_file_crudo = 'datasets/competencia_01.csv'\n",
        "dataset_file_fe = 'datasets/competencia_01_fe.csv'\n",
        "modelos_path = base_path + 'modelos/'\n",
        "db_path = base_path + 'db/'\n",
        "\n",
        "ganancia_acierto = 273000\n",
        "costo_estimulo = 7000\n",
        "\n",
        "mes_train = 202104\n",
        "mes_test = 202106\n",
        "\n",
        "# agregue sus semillas\n",
        "semillas = [211777, 174989, 131497, 612223, 234803]\n",
        "\n",
        "data = pd.read_csv(base_path + dataset_file_fe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#C. Funciones.\n",
        "def lgb_gan_eval(y_pred, data):\n",
        "    weight = data.get_weight()\n",
        "    ganancia = np.where(weight == 1.00002, ganancia_acierto, 0) - np.where(weight < 1.00002, costo_estimulo, 0)\n",
        "    ganancia = ganancia[np.argsort(y_pred)[::-1]]\n",
        "    ganancia = np.cumsum(ganancia)\n",
        "\n",
        "    return 'gan_eval', np.max(ganancia) , True\n",
        "\n",
        "def objective(trial): \n",
        "    # Rango de parámetros a buscar sus valores óptimos.\n",
        "    num_leaves = trial.suggest_int('num_leaves', 10, 200)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 0.005, 0.3) # mas bajo, más iteraciones necesita\n",
        "    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 50, 700)\n",
        "    feature_fraction = trial.suggest_float('feature_fraction', 0.1, 1.0)\n",
        "    bagging_fraction = trial.suggest_float('bagging_fraction', 0.1, 1.0)\n",
        "\n",
        "\n",
        "    # Parámetros que le voy a pasar al modelo.\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'custom',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'first_metric_only': True,\n",
        "        'boost_from_average': True,\n",
        "        'feature_pre_filter': False,\n",
        "        'max_bin': 31,\n",
        "        'num_leaves': num_leaves,\n",
        "        'learning_rate': learning_rate,\n",
        "        'min_data_in_leaf': min_data_in_leaf,\n",
        "        'feature_fraction': feature_fraction,\n",
        "        'bagging_fraction': bagging_fraction,\n",
        "        'seed': semillas[0],\n",
        "        'verbose': -1\n",
        "    }\n",
        "    \n",
        "    # Creo el dataset para Light GBM.\n",
        "    train_data = lgb.Dataset(X_train,\n",
        "                              label=y_train_binaria2, # eligir la clase\n",
        "                              weight=w_train)\n",
        "    \n",
        "    # Entreno.\n",
        "    cv_results = lgb.cv(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=100, # modificar, subit y subir... y descomentar la línea inferior\n",
        "        #callbacks=[lgb.early_stopping(int(50 + 5 / learning_rate))],\n",
        "        feval=lgb_gan_eval,\n",
        "        stratified=True,\n",
        "        nfold=5,\n",
        "        seed=semillas[0]\n",
        "    )\n",
        "    \n",
        "    # Calculo la ganancia máxima y la mejor iteración donde se obtuvo dicha ganancia.\n",
        "    max_gan = max(cv_results['valid gan_eval-mean'])\n",
        "    best_iter = cv_results['valid gan_eval-mean'].index(max_gan) + 1\n",
        "\n",
        "    # Guardamos cual es la mejor iteración del modelo\n",
        "    trial.set_user_attr(\"best_iter\", best_iter)\n",
        "\n",
        "    return max_gan * 5\n",
        "\n",
        "def ganancia_prob(y_pred, y_true, prop = 1):\n",
        "  ganancia = np.where(y_true == 1, ganancia_acierto, 0) - np.where(y_true == 0, costo_estimulo, 0)\n",
        "  return ganancia[y_pred >= 0.025].sum() / prop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#D. Pesos y reclusterización.\n",
        "data['clase_peso'] = 1.0\n",
        "\n",
        "data.loc[data['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
        "data.loc[data['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001\n",
        "\n",
        "data['clase_binaria1'] = 0\n",
        "data['clase_binaria2'] = 0\n",
        "data['clase_binaria1'] = np.where(data['clase_ternaria'] == 'BAJA+2', 1, 0)\n",
        "data['clase_binaria2'] = np.where(data['clase_ternaria'] == 'CONTINUA', 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#E. Divido entre Train y Test.\n",
        "train_data = data[data['foto_mes'] == mes_train]\n",
        "test_data = data[data['foto_mes'] == mes_test]\n",
        "\n",
        "X_train = train_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_train_binaria1 = train_data['clase_binaria1']\n",
        "y_train_binaria2 = train_data['clase_binaria2']\n",
        "w_train = train_data['clase_peso']\n",
        "\n",
        "X_test = test_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_test_binaria1 = test_data['clase_binaria1']\n",
        "y_test_class = test_data['clase_ternaria']\n",
        "w_test = test_data['clase_peso']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#F. Voy a realizar un estudio de Optuna para encontrar los mejores parámetros.\n",
        "#i. Creo la base de datos donde guardar los resultados.\n",
        "storage_name = \"sqlite:///\" + db_path + \"optimization_lgbm.db\"\n",
        "study_name = \"exp_304_lgbm_datos_fe_train_solo_abril\"\n",
        "\n",
        "#ii. Creo el estudio.\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=study_name,\n",
        "    storage=storage_name,\n",
        "    load_if_exists=True,\n",
        ")\n",
        "\n",
        "#iii. Corro el estudio.\n",
        "study.optimize(objective, n_trials=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#G. Visualizo los resultados del estudio, para modificar los rangos de análisis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optuna.visualization.plot_optimization_history(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_param_importances(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_slice(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_contour(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_contour(study, params=['num_leaves','min_data_in_leaf'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "study.best_trial.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#H. Tomamos el mejor modelo y con eso entrenamos todos los datos.\n",
        "best_iter = study.best_trial.user_attrs[\"best_iter\"]\n",
        "print(f\"Mejor cantidad de árboles para el mejor model {best_iter}\")\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'first_metric_only': True,\n",
        "    'boost_from_average': True,\n",
        "    'feature_pre_filter': False,\n",
        "    'max_bin': 31,\n",
        "    'num_leaves': study.best_trial.params['num_leaves'],\n",
        "    'learning_rate': study.best_trial.params['learning_rate'],\n",
        "    'min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],\n",
        "    'feature_fraction': study.best_trial.params['feature_fraction'],\n",
        "    'bagging_fraction': study.best_trial.params['bagging_fraction'],\n",
        "    'num_leaves': study.best_trial.params['num_leaves'],\n",
        "    'seed': semillas[0],\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "train_data = lgb.Dataset(X_train,\n",
        "                          label=y_train_binaria2,\n",
        "                          weight=w_train)\n",
        "\n",
        "model_lgb = lgb.train(params,\n",
        "                  train_data,\n",
        "                  num_boost_round=best_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#I. Observamos las variables más importantes para el modelo.\n",
        "lgb.plot_importance(model_lgb, figsize=(10, 20))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#J. Guardamos el modelo.\n",
        "model_lgb.save_model('./lgb_datos_fe_03_train_solo_abril_optimizado_optuna.txt')\n",
        "#model_lgb.save_model(modelos_path + 'lgb_datos_fe_02_ajusto_optuna.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#K. Volvemos a leer el modelo.\n",
        "model_lgb = lgb.Booster(model_file=\"./lgb_datos_fe_03_train_solo_abril_optimizado_optuna.txt\")\n",
        "#model_lgb = lgb.Booster(model_file=modelos_path + 'lgb_datos_fe_02_ajusto_optuna.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#a. Importo librería.\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "#b. Configura el API de Kaggle\n",
        "api = KaggleApi()\n",
        "api.authenticate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#L. Predecimos Junio.\n",
        "#i. Predecimos propiamente dicho.\n",
        "#predicciones = model_lgb.predict(X_test)\n",
        "#ii. Le pegamos la probabilidad de ser \"BAJA\" a cada cliente.\n",
        "#X_test['Probabilidad'] = predicciones\n",
        "#iii. Ordenamos a los clientes por probabilidad de ser \"BAJA\" de forma descendente.\n",
        "#tb_entrega = X_test.sort_values(by='Probabilidad', ascending=False)\n",
        "#iv. Genero una lista de distintos cortes candidatos, para enviar a Kaggle.\n",
        "cortes = range(13000,14000,200)\n",
        "#v. Generamos las distintas predicciones de clases a partir de los distintos cortes posibles.\n",
        "num_subida_kaggle = 41\n",
        "for envios in cortes:\n",
        "    #1. Le ponemos clase 1 (\"BAJA\") a los primeros \"envios\" con mayor probabilidad.\n",
        "    tb_entrega['Predicted'] = 0\n",
        "    tb_entrega.iloc[:envios, tb_entrega.columns.get_loc('Predicted')] = 1\n",
        "    resultados = tb_entrega[[\"numero_de_cliente\", 'Predicted']].reset_index(drop=True)\n",
        "    \n",
        "    print(\"Cantidad de clientes {}\".format(envios))\n",
        "    #2. Guardamos el archivo para Kaggle.\n",
        "    nombre_archivo = \"K106_S4A_00{}.csv\".format(num_subida_kaggle)\n",
        "    ruta_archivo= \"../../../exp/{}\".format(nombre_archivo)\n",
        "    resultados.to_csv(ruta_archivo, index=False)\n",
        "    \n",
        "    num_subida_kaggle += 1\n",
        "    \n",
        "    #3. Envío a Kaggle.\n",
        "    #a. Defino los parámetros claves.\n",
        "    mensaje = f'Archivo {nombre_archivo}. LGB Train Abril FE (lgb_datos_fe_03_train_solo_abril) optimizado OPTUNA, punto_corte: {envios}. No se imputa, 100 Trials para búsqueda de hiperparámetros, 100 boost con stop early'\n",
        "    competencia = 'dm-ey-f-2024-primera'\n",
        "    #c. Subo la Submission.\n",
        "    while True:\n",
        "        try:\n",
        "            api.competition_submit(file_name=ruta_archivo, message=mensaje, competition=competencia)\n",
        "            print(\"Submission successful!\")\n",
        "            break\n",
        "        except ApiException as e:\n",
        "            if e.status == 429:  # Too Many Requests\n",
        "                print(\"Rate limit exceeded. Retrying after 30 seconds...\")\n",
        "                time.sleep(30)\n",
        "            else:\n",
        "                raise e  # Re-raise other exceptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### B. Solo Marzo y Abril (drifting con -2 meses)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#A. Librerías.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "import optuna\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_slice, plot_contour\n",
        "\n",
        "from time import time\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#B. Constantes.\n",
        "base_path = 'C:/Users/c678456/Desktop/Ian/Maestría/Especializacion/2do_cuatrimestre/DMEyF/'\n",
        "dataset_file_crudo = 'datasets/competencia_01.csv'\n",
        "dataset_file_fe = 'datasets/competencia_01_fe_drifting_menos_2.csv'\n",
        "modelos_path = base_path + 'modelos/'\n",
        "db_path = base_path + 'db/'\n",
        "\n",
        "ganancia_acierto = 273000\n",
        "costo_estimulo = 7000\n",
        "\n",
        "mes_train = [202103,202104]\n",
        "mes_test = 202106\n",
        "\n",
        "# agregue sus semillas\n",
        "semillas = [211777, 174989, 131497, 612223, 234803]\n",
        "\n",
        "data = pd.read_csv(base_path + dataset_file_fe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#C. Funciones.\n",
        "def lgb_gan_eval(y_pred, data):\n",
        "    weight = data.get_weight()\n",
        "    ganancia = np.where(weight == 1.00002, ganancia_acierto, 0) - np.where(weight < 1.00002, costo_estimulo, 0)\n",
        "    ganancia = ganancia[np.argsort(y_pred)[::-1]]\n",
        "    ganancia = np.cumsum(ganancia)\n",
        "\n",
        "    return 'gan_eval', np.max(ganancia) , True\n",
        "\n",
        "def objective(trial): \n",
        "    # Rango de parámetros a buscar sus valores óptimos.\n",
        "    num_leaves = trial.suggest_int('num_leaves', 10, 200)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 0.005, 0.3) # mas bajo, más iteraciones necesita\n",
        "    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 50, 700)\n",
        "    feature_fraction = trial.suggest_float('feature_fraction', 0.1, 1.0)\n",
        "    bagging_fraction = trial.suggest_float('bagging_fraction', 0.1, 1.0)\n",
        "\n",
        "\n",
        "    # Parámetros que le voy a pasar al modelo.\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'custom',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'first_metric_only': True,\n",
        "        'boost_from_average': True,\n",
        "        'feature_pre_filter': False,\n",
        "        'max_bin': 31,\n",
        "        'num_leaves': num_leaves,\n",
        "        'learning_rate': learning_rate,\n",
        "        'min_data_in_leaf': min_data_in_leaf,\n",
        "        'feature_fraction': feature_fraction,\n",
        "        'bagging_fraction': bagging_fraction,\n",
        "        'seed': semillas[0],\n",
        "        'verbose': -1\n",
        "    }\n",
        "    \n",
        "    # Creo el dataset para Light GBM.\n",
        "    train_data = lgb.Dataset(X_train,\n",
        "                              label=y_train_binaria2, # eligir la clase\n",
        "                              weight=w_train)\n",
        "    \n",
        "    # Entreno.\n",
        "    cv_results = lgb.cv(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=100, # modificar, subit y subir... y descomentar la línea inferior\n",
        "        #callbacks=[lgb.early_stopping(int(50 + 5 / learning_rate))],\n",
        "        feval=lgb_gan_eval,\n",
        "        stratified=True,\n",
        "        nfold=5,\n",
        "        seed=semillas[0]\n",
        "    )\n",
        "    \n",
        "    # Calculo la ganancia máxima y la mejor iteración donde se obtuvo dicha ganancia.\n",
        "    max_gan = max(cv_results['valid gan_eval-mean'])\n",
        "    best_iter = cv_results['valid gan_eval-mean'].index(max_gan) + 1\n",
        "\n",
        "    # Guardamos cual es la mejor iteración del modelo\n",
        "    trial.set_user_attr(\"best_iter\", best_iter)\n",
        "\n",
        "    return max_gan * 5\n",
        "\n",
        "def ganancia_prob(y_pred, y_true, prop = 1):\n",
        "  ganancia = np.where(y_true == 1, ganancia_acierto, 0) - np.where(y_true == 0, costo_estimulo, 0)\n",
        "  return ganancia[y_pred >= 0.025].sum() / prop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#D. Pesos y reclusterización.\n",
        "data['clase_peso'] = 1.0\n",
        "\n",
        "data.loc[data['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
        "data.loc[data['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001\n",
        "\n",
        "data['clase_binaria1'] = 0\n",
        "data['clase_binaria2'] = 0\n",
        "data['clase_binaria1'] = np.where(data['clase_ternaria'] == 'BAJA+2', 1, 0)\n",
        "data['clase_binaria2'] = np.where(data['clase_ternaria'] == 'CONTINUA', 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#E. Divido entre Train y Test.\n",
        "train_data = data[data['foto_mes'].isin(mes_train)]\n",
        "test_data = data[data['foto_mes'] == mes_test]\n",
        "\n",
        "X_train = train_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_train_binaria1 = train_data['clase_binaria1']\n",
        "y_train_binaria2 = train_data['clase_binaria2']\n",
        "w_train = train_data['clase_peso']\n",
        "\n",
        "X_test = test_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_test_binaria1 = test_data['clase_binaria1']\n",
        "y_test_class = test_data['clase_ternaria']\n",
        "w_test = test_data['clase_peso']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#F. Voy a realizar un estudio de Optuna para encontrar los mejores parámetros.\n",
        "#i. Creo la base de datos donde guardar los resultados.\n",
        "storage_name = \"sqlite:///\" + db_path + \"optimization_lgbm.db\"\n",
        "study_name = \"exp_305_lgbm_datos_fe_train_marzo_abril_drifting_menos_2\"\n",
        "\n",
        "#ii. Creo el estudio.\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=study_name,\n",
        "    storage=storage_name,\n",
        "    load_if_exists=True,\n",
        ")\n",
        "\n",
        "#iii. Corro el estudio.\n",
        "study.optimize(objective, n_trials=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#G. Visualizo los resultados del estudio, para modificar los rangos de análisis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optuna.visualization.plot_optimization_history(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_param_importances(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_slice(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_contour(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_contour(study, params=['num_leaves','min_data_in_leaf'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "study.best_trial.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#H. Tomamos el mejor modelo y con eso entrenamos todos los datos.\n",
        "best_iter = study.best_trial.user_attrs[\"best_iter\"]\n",
        "print(f\"Mejor cantidad de árboles para el mejor model {best_iter}\")\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'first_metric_only': True,\n",
        "    'boost_from_average': True,\n",
        "    'feature_pre_filter': False,\n",
        "    'max_bin': 31,\n",
        "    'num_leaves': study.best_trial.params['num_leaves'],\n",
        "    'learning_rate': study.best_trial.params['learning_rate'],\n",
        "    'min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],\n",
        "    'feature_fraction': study.best_trial.params['feature_fraction'],\n",
        "    'bagging_fraction': study.best_trial.params['bagging_fraction'],\n",
        "    'num_leaves': study.best_trial.params['num_leaves'],\n",
        "    'seed': semillas[0],\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "train_data = lgb.Dataset(X_train,\n",
        "                          label=y_train_binaria2,\n",
        "                          weight=w_train)\n",
        "\n",
        "model_lgb = lgb.train(params,\n",
        "                  train_data,\n",
        "                  num_boost_round=best_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#I. Observamos las variables más importantes para el modelo.\n",
        "lgb.plot_importance(model_lgb, figsize=(10, 20))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener los nombres de las características\n",
        "features = X_test.columns\n",
        "\n",
        "# Obtener la importancia de las características\n",
        "importancias = model_lgb.feature_importance()\n",
        "\n",
        "# Crear un DataFrame con los nombres de las características y su importancia\n",
        "df_importancia = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': importancias\n",
        "})\n",
        "\n",
        "# Ordenar el DataFrame por la importancia de las características en orden descendente\n",
        "df_importancia = df_importancia.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "df_importancia.head(65)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#J. Guardamos el modelo.\n",
        "model_lgb.save_model('./lgb_datos_fe_04_train_solo_marzo_abril_drifting_menos_2.txt')\n",
        "#model_lgb.save_model(modelos_path + 'lgb_datos_fe_02_ajusto_optuna.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "#K. Volvemos a leer el modelo.\n",
        "model_lgb = lgb.Booster(model_file=\"./lgb_datos_fe_04_train_solo_marzo_abril_drifting_menos_2.txt\")\n",
        "#model_lgb = lgb.Booster(model_file=modelos_path + 'lgb_datos_fe_02_ajusto_optuna.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#a. Importo librería.\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "#b. Configura el API de Kaggle\n",
        "api = KaggleApi()\n",
        "api.authenticate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#L. Predecimos Junio.\n",
        "#i. Predecimos propiamente dicho.\n",
        "#predicciones = model_lgb.predict(X_test)\n",
        "#ii. Le pegamos la probabilidad de ser \"BAJA\" a cada cliente.\n",
        "#X_test['Probabilidad'] = predicciones\n",
        "#iii. Ordenamos a los clientes por probabilidad de ser \"BAJA\" de forma descendente.\n",
        "#tb_entrega = X_test.sort_values(by='Probabilidad', ascending=False)\n",
        "#iv. Genero una lista de distintos cortes candidatos, para enviar a Kaggle.\n",
        "cortes = range(9000,14000,200)\n",
        "#v. Generamos las distintas predicciones de clases a partir de los distintos cortes posibles.\n",
        "num_subida_kaggle = 1\n",
        "for envios in cortes:\n",
        "    #1. Le ponemos clase 1 (\"BAJA\") a los primeros \"envios\" con mayor probabilidad.\n",
        "    tb_entrega['Predicted'] = 0\n",
        "    tb_entrega.iloc[:envios, tb_entrega.columns.get_loc('Predicted')] = 1\n",
        "    resultados = tb_entrega[[\"numero_de_cliente\", 'Predicted']].reset_index(drop=True)\n",
        "    \n",
        "    print(\"Cantidad de clientes {}\".format(envios))\n",
        "    #2. Guardamos el archivo para Kaggle.\n",
        "    nombre_archivo = \"K106_S4B_00{}.csv\".format(num_subida_kaggle)\n",
        "    ruta_archivo= \"../../../exp/{}\".format(nombre_archivo)\n",
        "    resultados.to_csv(ruta_archivo, index=False)\n",
        "    \n",
        "    num_subida_kaggle += 1\n",
        "    \n",
        "    #3. Envío a Kaggle.\n",
        "    #a. Defino los parámetros claves.\n",
        "    mensaje = f'Archivo {nombre_archivo}. LGB Train Marzo-Abril FE (lgb_datos_fe_03_train_solo_marzo_abril_drifting_menos_2), punto_corte: {envios}. No se imputa, 100 Trials para búsqueda de hiperparámetros, 100 boost con stop early'\n",
        "    competencia = 'dm-ey-f-2024-primera'\n",
        "    #c. Subo la Submission.\n",
        "    while True:\n",
        "        try:\n",
        "            api.competition_submit(file_name=ruta_archivo, message=mensaje, competition=competencia)\n",
        "            print(\"Submission successful!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            if e.status == 429:  # Too Many Requests\n",
        "                print(\"Rate limit exceeded. Retrying after 30 seconds...\")\n",
        "                time.sleep(30)\n",
        "            else:\n",
        "                raise e  # Re-raise other exceptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### C. Solo Febrero, Marzo y Abril (drifting con -1 meses)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "#A. Librerías.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "import optuna\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_slice, plot_contour\n",
        "\n",
        "from time import time\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "#B. Constantes.\n",
        "base_path = 'C:/Users/c678456/Desktop/Ian/Maestría/Especializacion/2do_cuatrimestre/DMEyF/'\n",
        "dataset_file_crudo = 'datasets/competencia_01.csv'\n",
        "dataset_file_fe = 'datasets/competencia_01_fe_drifting_menos_1.csv'\n",
        "modelos_path = base_path + 'modelos/'\n",
        "db_path = base_path + 'db/'\n",
        "\n",
        "ganancia_acierto = 273000\n",
        "costo_estimulo = 7000\n",
        "\n",
        "mes_train = [202102,202103,202104]\n",
        "mes_test = 202106\n",
        "\n",
        "# agregue sus semillas\n",
        "semillas = [211777, 174989, 131497, 612223, 234803]\n",
        "\n",
        "data = pd.read_csv(base_path + dataset_file_fe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "#C. Funciones.\n",
        "def lgb_gan_eval(y_pred, data):\n",
        "    weight = data.get_weight()\n",
        "    ganancia = np.where(weight == 1.00002, ganancia_acierto, 0) - np.where(weight < 1.00002, costo_estimulo, 0)\n",
        "    ganancia = ganancia[np.argsort(y_pred)[::-1]]\n",
        "    ganancia = np.cumsum(ganancia)\n",
        "\n",
        "    return 'gan_eval', np.max(ganancia) , True\n",
        "\n",
        "def objective(trial): \n",
        "    # Rango de parámetros a buscar sus valores óptimos.\n",
        "    num_leaves = trial.suggest_int('num_leaves', 10, 200)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 0.005, 0.3) # mas bajo, más iteraciones necesita\n",
        "    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 50, 700)\n",
        "    feature_fraction = trial.suggest_float('feature_fraction', 0.1, 1.0)\n",
        "    bagging_fraction = trial.suggest_float('bagging_fraction', 0.1, 1.0)\n",
        "\n",
        "\n",
        "    # Parámetros que le voy a pasar al modelo.\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'custom',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'first_metric_only': True,\n",
        "        'boost_from_average': True,\n",
        "        'feature_pre_filter': False,\n",
        "        'max_bin': 31,\n",
        "        'num_leaves': num_leaves,\n",
        "        'learning_rate': learning_rate,\n",
        "        'min_data_in_leaf': min_data_in_leaf,\n",
        "        'feature_fraction': feature_fraction,\n",
        "        'bagging_fraction': bagging_fraction,\n",
        "        'seed': semillas[0],\n",
        "        'verbose': -1\n",
        "    }\n",
        "    \n",
        "    # Creo el dataset para Light GBM.\n",
        "    train_data = lgb.Dataset(X_train,\n",
        "                              label=y_train_binaria2, # eligir la clase\n",
        "                              weight=w_train)\n",
        "    \n",
        "    # Entreno.\n",
        "    cv_results = lgb.cv(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=100, # modificar, subit y subir... y descomentar la línea inferior\n",
        "        #callbacks=[lgb.early_stopping(int(50 + 5 / learning_rate))],\n",
        "        feval=lgb_gan_eval,\n",
        "        stratified=True,\n",
        "        nfold=5,\n",
        "        seed=semillas[0]\n",
        "    )\n",
        "    \n",
        "    # Calculo la ganancia máxima y la mejor iteración donde se obtuvo dicha ganancia.\n",
        "    max_gan = max(cv_results['valid gan_eval-mean'])\n",
        "    best_iter = cv_results['valid gan_eval-mean'].index(max_gan) + 1\n",
        "\n",
        "    # Guardamos cual es la mejor iteración del modelo\n",
        "    trial.set_user_attr(\"best_iter\", best_iter)\n",
        "\n",
        "    return max_gan * 5\n",
        "\n",
        "def ganancia_prob(y_pred, y_true, prop = 1):\n",
        "  ganancia = np.where(y_true == 1, ganancia_acierto, 0) - np.where(y_true == 0, costo_estimulo, 0)\n",
        "  return ganancia[y_pred >= 0.025].sum() / prop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "#D. Pesos y reclusterización.\n",
        "data['clase_peso'] = 1.0\n",
        "\n",
        "data.loc[data['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
        "data.loc[data['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001\n",
        "\n",
        "data['clase_binaria1'] = 0\n",
        "data['clase_binaria2'] = 0\n",
        "data['clase_binaria1'] = np.where(data['clase_ternaria'] == 'BAJA+2', 1, 0)\n",
        "data['clase_binaria2'] = np.where(data['clase_ternaria'] == 'CONTINUA', 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "#E. Divido entre Train y Test.\n",
        "train_data = data[data['foto_mes'].isin(mes_train)]\n",
        "test_data = data[data['foto_mes'] == mes_test]\n",
        "\n",
        "X_train = train_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_train_binaria1 = train_data['clase_binaria1']\n",
        "y_train_binaria2 = train_data['clase_binaria2']\n",
        "w_train = train_data['clase_peso']\n",
        "\n",
        "X_test = test_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_test_binaria1 = test_data['clase_binaria1']\n",
        "y_test_class = test_data['clase_ternaria']\n",
        "w_test = test_data['clase_peso']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#F. Voy a realizar un estudio de Optuna para encontrar los mejores parámetros.\n",
        "#i. Creo la base de datos donde guardar los resultados.\n",
        "storage_name = \"sqlite:///\" + db_path + \"optimization_lgbm.db\"\n",
        "study_name = \"exp_306_lgbm_datos_fe_train_febrero_marzo_abril_drifting_menos_1\"\n",
        "\n",
        "#ii. Creo el estudio.\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=study_name,\n",
        "    storage=storage_name,\n",
        "    load_if_exists=True,\n",
        ")\n",
        "\n",
        "#iii. Corro el estudio.\n",
        "study.optimize(objective, n_trials=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "#G. Visualizo los resultados del estudio, para modificar los rangos de análisis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optuna.visualization.plot_optimization_history(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_param_importances(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_slice(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_contour(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_contour(study, params=['num_leaves','min_data_in_leaf'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "study.best_trial.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#H. Tomamos el mejor modelo y con eso entrenamos todos los datos.\n",
        "best_iter = study.best_trial.user_attrs[\"best_iter\"]\n",
        "print(f\"Mejor cantidad de árboles para el mejor model {best_iter}\")\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'first_metric_only': True,\n",
        "    'boost_from_average': True,\n",
        "    'feature_pre_filter': False,\n",
        "    'max_bin': 31,\n",
        "    'num_leaves': study.best_trial.params['num_leaves'],\n",
        "    'learning_rate': study.best_trial.params['learning_rate'],\n",
        "    'min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],\n",
        "    'feature_fraction': study.best_trial.params['feature_fraction'],\n",
        "    'bagging_fraction': study.best_trial.params['bagging_fraction'],\n",
        "    'num_leaves': study.best_trial.params['num_leaves'],\n",
        "    'seed': semillas[0],\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "train_data = lgb.Dataset(X_train,\n",
        "                          label=y_train_binaria2,\n",
        "                          weight=w_train)\n",
        "\n",
        "model_lgb = lgb.train(params,\n",
        "                  train_data,\n",
        "                  num_boost_round=best_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#I. Observamos las variables más importantes para el modelo.\n",
        "lgb.plot_importance(model_lgb, figsize=(10, 20))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener los nombres de las características\n",
        "features = X_test.columns\n",
        "\n",
        "# Obtener la importancia de las características\n",
        "importancias = model_lgb.feature_importance()\n",
        "\n",
        "# Crear un DataFrame con los nombres de las características y su importancia\n",
        "df_importancia = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': importancias\n",
        "})\n",
        "\n",
        "# Ordenar el DataFrame por la importancia de las características en orden descendente\n",
        "df_importancia = df_importancia.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "df_importancia.head(65)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#J. Guardamos el modelo.\n",
        "model_lgb.save_model('./lgb_datos_fe_05_train_solo_febrero_marzo_abril_drifting_menos_1.txt')\n",
        "#model_lgb.save_model(modelos_path + 'lgb_datos_fe_02_ajusto_optuna.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "#K. Volvemos a leer el modelo.\n",
        "model_lgb = lgb.Booster(model_file=\"./lgb_datos_fe_05_train_solo_febrero_marzo_abril_drifting_menos_1.txt\")\n",
        "#model_lgb = lgb.Booster(model_file=modelos_path + 'lgb_datos_fe_02_ajusto_optuna.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "#a. Importo librería.\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "#b. Configura el API de Kaggle\n",
        "api = KaggleApi()\n",
        "api.authenticate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#L. Predecimos Junio.\n",
        "#i. Predecimos propiamente dicho.\n",
        "predicciones = model_lgb.predict(X_test)\n",
        "#ii. Le pegamos la probabilidad de ser \"BAJA\" a cada cliente.\n",
        "X_test['Probabilidad'] = predicciones\n",
        "#iii. Ordenamos a los clientes por probabilidad de ser \"BAJA\" de forma descendente.\n",
        "tb_entrega = X_test.sort_values(by='Probabilidad', ascending=False)\n",
        "#iv. Genero una lista de distintos cortes candidatos, para enviar a Kaggle.\n",
        "cortes = range(7000,9000,200)\n",
        "#v. Generamos las distintas predicciones de clases a partir de los distintos cortes posibles.\n",
        "num_subida_kaggle = 26\n",
        "for envios in cortes:\n",
        "    #1. Le ponemos clase 1 (\"BAJA\") a los primeros \"envios\" con mayor probabilidad.\n",
        "    tb_entrega['Predicted'] = 0\n",
        "    tb_entrega.iloc[:envios, tb_entrega.columns.get_loc('Predicted')] = 1\n",
        "    resultados = tb_entrega[[\"numero_de_cliente\", 'Predicted']].reset_index(drop=True)\n",
        "    \n",
        "    print(\"Cantidad de clientes {}\".format(envios))\n",
        "    #2. Guardamos el archivo para Kaggle.\n",
        "    nombre_archivo = \"K106_S4C_00{}.csv\".format(num_subida_kaggle)\n",
        "    ruta_archivo= \"../../../exp/{}\".format(nombre_archivo)\n",
        "    resultados.to_csv(ruta_archivo, index=False)\n",
        "    \n",
        "    num_subida_kaggle += 1\n",
        "    \n",
        "    #3. Envío a Kaggle.\n",
        "    #a. Defino los parámetros claves.\n",
        "    mensaje = f'Archivo {nombre_archivo}. LGB Train Febrero-Marzo-Abril FE (lgb_datos_fe_05_train_solo_febrero_marzo_abril_drifting_menos_1), punto_corte: {envios}. No se imputa, 100 Trials para búsqueda de hiperparámetros, 100 boost con stop early'\n",
        "    competencia = 'dm-ey-f-2024-primera'\n",
        "    #c. Subo la Submission.\n",
        "    while True:\n",
        "        try:\n",
        "            api.competition_submit(file_name=ruta_archivo, message=mensaje, competition=competencia)\n",
        "            print(\"Submission successful!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            if e.status == 429:  # Too Many Requests\n",
        "                print(\"Rate limit exceeded. Retrying after 30 seconds...\")\n",
        "                time.sleep(30)\n",
        "            else:\n",
        "                raise e  # Re-raise other exceptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Pruebo LGBM con datos de entrenamiento de Abril con lag -3 en data drifting y con nuevas variables de data drifting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#A. Librerías.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "import optuna\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_slice, plot_contour\n",
        "\n",
        "from time import time\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#B. Constantes.\n",
        "base_path = 'C:/Users/c678456/Desktop/Ian/Maestría/Especializacion/2do_cuatrimestre/DMEyF/'\n",
        "dataset_file_crudo = 'datasets/competencia_01.csv'\n",
        "dataset_file_fe = 'datasets/competencia_01_fe_menos_3_con_ratios.csv'\n",
        "modelos_path = base_path + 'modelos/'\n",
        "db_path = base_path + 'db/'\n",
        "\n",
        "ganancia_acierto = 273000\n",
        "costo_estimulo = 7000\n",
        "\n",
        "mes_train = 202104\n",
        "mes_test = 202106\n",
        "\n",
        "# agregue sus semillas\n",
        "semillas = [211777, 174989, 131497, 612223, 234803]\n",
        "\n",
        "data = pd.read_csv(base_path + dataset_file_fe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#C. Funciones.\n",
        "def lgb_gan_eval(y_pred, data):\n",
        "    weight = data.get_weight()\n",
        "    ganancia = np.where(weight == 1.00002, ganancia_acierto, 0) - np.where(weight < 1.00002, costo_estimulo, 0)\n",
        "    ganancia = ganancia[np.argsort(y_pred)[::-1]]\n",
        "    ganancia = np.cumsum(ganancia)\n",
        "\n",
        "    return 'gan_eval', np.max(ganancia) , True\n",
        "\n",
        "def objective(trial): \n",
        "    # Rango de parámetros a buscar sus valores óptimos.\n",
        "    num_leaves = trial.suggest_int('num_leaves', 10, 200)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 0.005, 0.3) # mas bajo, más iteraciones necesita\n",
        "    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 50, 1500)\n",
        "    feature_fraction = trial.suggest_float('feature_fraction', 0.1, 1.0)\n",
        "    bagging_fraction = trial.suggest_float('bagging_fraction', 0.1, 1.0)\n",
        "\n",
        "\n",
        "    # Parámetros que le voy a pasar al modelo.\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'custom',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'first_metric_only': True,\n",
        "        'boost_from_average': True,\n",
        "        'feature_pre_filter': False,\n",
        "        'max_bin': 31,\n",
        "        'num_leaves': num_leaves,\n",
        "        'learning_rate': learning_rate,\n",
        "        'min_data_in_leaf': min_data_in_leaf,\n",
        "        'feature_fraction': feature_fraction,\n",
        "        'bagging_fraction': bagging_fraction,\n",
        "        'seed': semillas[0],\n",
        "        'verbose': -1\n",
        "    }\n",
        "    \n",
        "    # Creo el dataset para Light GBM.\n",
        "    train_data = lgb.Dataset(X_train,\n",
        "                              label=y_train_binaria2, # eligir la clase\n",
        "                              weight=w_train)\n",
        "    \n",
        "    # Entreno.\n",
        "    cv_results = lgb.cv(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=1000, # modificar, subit y subir... y descomentar la línea inferior\n",
        "        callbacks=[lgb.early_stopping(int(50 + 5 / learning_rate))],\n",
        "        feval=lgb_gan_eval,\n",
        "        stratified=True,\n",
        "        nfold=5,\n",
        "        seed=semillas[0]\n",
        "    )\n",
        "    \n",
        "    # Calculo la ganancia máxima y la mejor iteración donde se obtuvo dicha ganancia.\n",
        "    max_gan = max(cv_results['valid gan_eval-mean'])\n",
        "    best_iter = cv_results['valid gan_eval-mean'].index(max_gan) + 1\n",
        "\n",
        "    # Guardamos cual es la mejor iteración del modelo\n",
        "    trial.set_user_attr(\"best_iter\", best_iter)\n",
        "\n",
        "    return max_gan * 5\n",
        "\n",
        "def ganancia_prob(y_pred, y_true, prop = 1):\n",
        "  ganancia = np.where(y_true == 1, ganancia_acierto, 0) - np.where(y_true == 0, costo_estimulo, 0)\n",
        "  return ganancia[y_pred >= 0.025].sum() / prop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#D. Pesos y reclusterización.\n",
        "data['clase_peso'] = 1.0\n",
        "\n",
        "data.loc[data['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
        "data.loc[data['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001\n",
        "\n",
        "data['clase_binaria1'] = 0\n",
        "data['clase_binaria2'] = 0\n",
        "data['clase_binaria1'] = np.where(data['clase_ternaria'] == 'BAJA+2', 1, 0)\n",
        "data['clase_binaria2'] = np.where(data['clase_ternaria'] == 'CONTINUA', 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#E. Divido entre Train y Test.\n",
        "train_data = data[data['foto_mes'] == mes_train]\n",
        "test_data = data[data['foto_mes'] == mes_test]\n",
        "\n",
        "X_train = train_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_train_binaria1 = train_data['clase_binaria1']\n",
        "y_train_binaria2 = train_data['clase_binaria2']\n",
        "w_train = train_data['clase_peso']\n",
        "\n",
        "X_test = test_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_test_binaria1 = test_data['clase_binaria1']\n",
        "y_test_class = test_data['clase_ternaria']\n",
        "w_test = test_data['clase_peso']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#F. Voy a realizar un estudio de Optuna para encontrar los mejores parámetros.\n",
        "#i. Creo la base de datos donde guardar los resultados.\n",
        "storage_name = \"sqlite:///\" + db_path + \"optimization_lgbm.db\"\n",
        "study_name = \"exp_307_lgbm_datos_fe_train_solo_abril_con_ratios\"\n",
        "\n",
        "#ii. Creo el estudio.\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=study_name,\n",
        "    storage=storage_name,\n",
        "    load_if_exists=True,\n",
        ")\n",
        "\n",
        "#iii. Corro el estudio.\n",
        "study.optimize(objective, n_trials=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Finalizó\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#G. Visualizo los resultados del estudio, para modificar los rangos de análisis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optuna.visualization.plot_optimization_history(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_param_importances(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_slice(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# min_data_in_leaf se podría subir un poco más."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_contour(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_contour(study, params=['num_leaves','min_data_in_leaf'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "study.best_trial.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#H. Tomamos el mejor modelo y con eso entrenamos todos los datos.\n",
        "best_iter = study.best_trial.user_attrs[\"best_iter\"]\n",
        "print(f\"Mejor cantidad de árboles para el mejor model {best_iter}\")\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'first_metric_only': True,\n",
        "    'boost_from_average': True,\n",
        "    'feature_pre_filter': False,\n",
        "    'max_bin': 31,\n",
        "    'num_leaves': study.best_trial.params['num_leaves'],\n",
        "    'learning_rate': study.best_trial.params['learning_rate'],\n",
        "    'min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],\n",
        "    'feature_fraction': study.best_trial.params['feature_fraction'],\n",
        "    'bagging_fraction': study.best_trial.params['bagging_fraction'],\n",
        "    'num_leaves': study.best_trial.params['num_leaves'],\n",
        "    'seed': semillas[0],\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "train_data = lgb.Dataset(X_train,\n",
        "                          label=y_train_binaria2,\n",
        "                          weight=w_train)\n",
        "\n",
        "model_lgb = lgb.train(params,\n",
        "                  train_data,\n",
        "                  num_boost_round=best_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#I. Observamos las variables más importantes para el modelo.\n",
        "lgb.plot_importance(model_lgb, figsize=(10, 20))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract feature importance and feature names\n",
        "importance = model_lgb.feature_importance()\n",
        "features = model_lgb.feature_name()\n",
        "\n",
        "# Create a dataframe for better visualization\n",
        "importance_df = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
        "\n",
        "# Sort by importance in descending order\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# show.\n",
        "importance_df.head(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#J. Guardamos el modelo.\n",
        "model_lgb.save_model('./lgb_datos_fe_05_train_solo_abril_con_ratios_optimizado_OPTUNA.txt')\n",
        "#model_lgb.save_model(modelos_path + 'lgb_datos_fe_02_ajusto_optuna.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#K. Volvemos a leer el modelo.\n",
        "model_lgb = lgb.Booster(model_file=\"./lgb_datos_fe_05_train_solo_abril_con_ratios_optimizado_OPTUNA.txt\")\n",
        "#model_lgb = lgb.Booster(model_file=modelos_path + 'lgb_datos_fe_02_ajusto_optuna.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "#a. Importo librería.\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "#b. Configura el API de Kaggle\n",
        "api = KaggleApi()\n",
        "api.authenticate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#L. Predecimos Junio.\n",
        "#i. Predecimos propiamente dicho.\n",
        "#predicciones = model_lgb.predict(X_test)\n",
        "#ii. Le pegamos la probabilidad de ser \"BAJA\" a cada cliente.\n",
        "#X_test['Probabilidad'] = predicciones\n",
        "#iii. Ordenamos a los clientes por probabilidad de ser \"BAJA\" de forma descendente.\n",
        "#tb_entrega = X_test.sort_values(by='Probabilidad', ascending=False)\n",
        "#iv. Genero una lista de distintos cortes candidatos, para enviar a Kaggle.\n",
        "cortes = range(13000,14000,200)\n",
        "#v. Generamos las distintas predicciones de clases a partir de los distintos cortes posibles.\n",
        "num_subida_kaggle = 20\n",
        "for envios in cortes:\n",
        "    #1. Le ponemos clase 1 (\"BAJA\") a los primeros \"envios\" con mayor probabilidad.\n",
        "    tb_entrega['Predicted'] = 0\n",
        "    tb_entrega.iloc[:envios, tb_entrega.columns.get_loc('Predicted')] = 1\n",
        "    resultados = tb_entrega[[\"numero_de_cliente\", 'Predicted']].reset_index(drop=True)\n",
        "    \n",
        "    print(\"Cantidad de clientes {}\".format(envios))\n",
        "    #2. Guardamos el archivo para Kaggle.\n",
        "    nombre_archivo = \"K106_S5A_00{}.csv\".format(num_subida_kaggle)\n",
        "    ruta_archivo= \"../../../exp/{}\".format(nombre_archivo)\n",
        "    resultados.to_csv(ruta_archivo, index=False)\n",
        "    \n",
        "    num_subida_kaggle += 1\n",
        "    \n",
        "    #3. Envío a Kaggle.\n",
        "    #a. Defino los parámetros claves.\n",
        "    mensaje = f'Archivo {nombre_archivo}. LGB Train Abril FE (lgb_datos_fe_05_train_solo_abril_con_ratios_optimizado_OPTUNA) optimizado OPTUNA, punto_corte: {envios}. No se imputa, 100 Trials para búsqueda de hiperparámetros, 1000 boost con stop early'\n",
        "    competencia = 'dm-ey-f-2024-primera'\n",
        "    #c. Subo la Submission.\n",
        "    while True:\n",
        "        try:\n",
        "            api.competition_submit(file_name=ruta_archivo, message=mensaje, competition=competencia)\n",
        "            print(\"Submission successful!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            if e.status == 429:  # Too Many Requests\n",
        "                print(\"Rate limit exceeded. Retrying after 30 seconds...\")\n",
        "                time.sleep(30)\n",
        "            else:\n",
        "                raise e  # Re-raise other exceptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Pruebo LGBM con datos de entrenamiento de Marzo y Abril con lag -2 en data drifting y con nuevas variables de data drifting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#A. Librerías.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "import optuna\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_slice, plot_contour\n",
        "\n",
        "from time import time\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#B. Constantes.\n",
        "base_path = 'C:/Users/c678456/Desktop/Ian/Maestría/Especializacion/2do_cuatrimestre/DMEyF/'\n",
        "dataset_file_crudo = 'datasets/competencia_01.csv'\n",
        "dataset_file_fe = 'datasets/competencia_01_fe_drifting_menos_2_con_ratios.csv'\n",
        "modelos_path = base_path + 'modelos/'\n",
        "db_path = base_path + 'db/'\n",
        "\n",
        "ganancia_acierto = 273000\n",
        "costo_estimulo = 7000\n",
        "\n",
        "mes_train = [202103,202104]\n",
        "mes_test = 202106\n",
        "\n",
        "# agregue sus semillas\n",
        "semillas = [211777, 174989, 131497, 612223, 234803]\n",
        "\n",
        "data = pd.read_csv(base_path + dataset_file_fe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#C. Funciones.\n",
        "def lgb_gan_eval(y_pred, data):\n",
        "    weight = data.get_weight()\n",
        "    ganancia = np.where(weight == 1.00002, ganancia_acierto, 0) - np.where(weight < 1.00002, costo_estimulo, 0)\n",
        "    ganancia = ganancia[np.argsort(y_pred)[::-1]]\n",
        "    ganancia = np.cumsum(ganancia)\n",
        "\n",
        "    return 'gan_eval', np.max(ganancia) , True\n",
        "\n",
        "def objective(trial): \n",
        "    # Rango de parámetros a buscar sus valores óptimos.\n",
        "    num_leaves = trial.suggest_int('num_leaves', 10, 200)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 0.005, 0.3) # mas bajo, más iteraciones necesita\n",
        "    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 50, 2000)\n",
        "    feature_fraction = trial.suggest_float('feature_fraction', 0.1, 1.0)\n",
        "    bagging_fraction = trial.suggest_float('bagging_fraction', 0.1, 1.0)\n",
        "\n",
        "\n",
        "    # Parámetros que le voy a pasar al modelo.\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'custom',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'first_metric_only': True,\n",
        "        'boost_from_average': True,\n",
        "        'feature_pre_filter': False,\n",
        "        'max_bin': 31,\n",
        "        'num_leaves': num_leaves,\n",
        "        'learning_rate': learning_rate,\n",
        "        'min_data_in_leaf': min_data_in_leaf,\n",
        "        'feature_fraction': feature_fraction,\n",
        "        'bagging_fraction': bagging_fraction,\n",
        "        'seed': semillas[0],\n",
        "        'verbose': -1\n",
        "    }\n",
        "    \n",
        "    # Creo el dataset para Light GBM.\n",
        "    train_data = lgb.Dataset(X_train,\n",
        "                              label=y_train_binaria2, # eligir la clase\n",
        "                              weight=w_train)\n",
        "    \n",
        "    # Entreno.\n",
        "    cv_results = lgb.cv(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=100, # modificar, subit y subir... y descomentar la línea inferior\n",
        "        #callbacks=[lgb.early_stopping(int(50 + 5 / learning_rate))],\n",
        "        feval=lgb_gan_eval,\n",
        "        stratified=True,\n",
        "        nfold=5,\n",
        "        seed=semillas[0]\n",
        "    )\n",
        "    \n",
        "    # Calculo la ganancia máxima y la mejor iteración donde se obtuvo dicha ganancia.\n",
        "    max_gan = max(cv_results['valid gan_eval-mean'])\n",
        "    best_iter = cv_results['valid gan_eval-mean'].index(max_gan) + 1\n",
        "\n",
        "    # Guardamos cual es la mejor iteración del modelo\n",
        "    trial.set_user_attr(\"best_iter\", best_iter)\n",
        "\n",
        "    return max_gan * 5\n",
        "\n",
        "def ganancia_prob(y_pred, y_true, prop = 1):\n",
        "  ganancia = np.where(y_true == 1, ganancia_acierto, 0) - np.where(y_true == 0, costo_estimulo, 0)\n",
        "  return ganancia[y_pred >= 0.025].sum() / prop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#D. Pesos y reclusterización.\n",
        "data['clase_peso'] = 1.0\n",
        "\n",
        "data.loc[data['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
        "data.loc[data['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001\n",
        "\n",
        "data['clase_binaria1'] = 0\n",
        "data['clase_binaria2'] = 0\n",
        "data['clase_binaria1'] = np.where(data['clase_ternaria'] == 'BAJA+2', 1, 0)\n",
        "data['clase_binaria2'] = np.where(data['clase_ternaria'] == 'CONTINUA', 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#E. Divido entre Train y Test.\n",
        "train_data = data[data['foto_mes'].isin(mes_train)]\n",
        "test_data = data[data['foto_mes'] == mes_test]\n",
        "\n",
        "X_train = train_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_train_binaria1 = train_data['clase_binaria1']\n",
        "y_train_binaria2 = train_data['clase_binaria2']\n",
        "w_train = train_data['clase_peso']\n",
        "\n",
        "X_test = test_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_test_binaria1 = test_data['clase_binaria1']\n",
        "y_test_class = test_data['clase_ternaria']\n",
        "w_test = test_data['clase_peso']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#F. Voy a realizar un estudio de Optuna para encontrar los mejores parámetros.\n",
        "#i. Creo la base de datos donde guardar los resultados.\n",
        "storage_name = \"sqlite:///\" + db_path + \"optimization_lgbm.db\"\n",
        "study_name = \"exp_308_lgbm_datos_fe_train_solo_marzo_abril_con_ratios\"\n",
        "\n",
        "#ii. Creo el estudio.\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=study_name,\n",
        "    storage=storage_name,\n",
        "    load_if_exists=True,\n",
        ")\n",
        "\n",
        "#iii. Corro el estudio.\n",
        "study.optimize(objective, n_trials=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Finalizó\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#G. Visualizo los resultados del estudio, para modificar los rangos de análisis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optuna.visualization.plot_optimization_history(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_param_importances(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_slice(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# min_data_in_leaf se podría subir un poco más."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_contour(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_contour(study, params=['num_leaves','min_data_in_leaf'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "study.best_trial.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#H. Tomamos el mejor modelo y con eso entrenamos todos los datos.\n",
        "best_iter = study.best_trial.user_attrs[\"best_iter\"]\n",
        "print(f\"Mejor cantidad de árboles para el mejor model {best_iter}\")\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'first_metric_only': True,\n",
        "    'boost_from_average': True,\n",
        "    'feature_pre_filter': False,\n",
        "    'max_bin': 31,\n",
        "    'num_leaves': study.best_trial.params['num_leaves'],\n",
        "    'learning_rate': study.best_trial.params['learning_rate'],\n",
        "    'min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],\n",
        "    'feature_fraction': study.best_trial.params['feature_fraction'],\n",
        "    'bagging_fraction': study.best_trial.params['bagging_fraction'],\n",
        "    'num_leaves': study.best_trial.params['num_leaves'],\n",
        "    'seed': semillas[0],\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "train_data = lgb.Dataset(X_train,\n",
        "                          label=y_train_binaria2,\n",
        "                          weight=w_train)\n",
        "\n",
        "model_lgb = lgb.train(params,\n",
        "                  train_data,\n",
        "                  num_boost_round=best_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#I. Observamos las variables más importantes para el modelo.\n",
        "lgb.plot_importance(model_lgb, figsize=(10, 20))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract feature importance and feature names\n",
        "importance = model_lgb.feature_importance()\n",
        "features = model_lgb.feature_name()\n",
        "\n",
        "# Create a dataframe for better visualization\n",
        "importance_df = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
        "\n",
        "# Sort by importance in descending order\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# show.\n",
        "importance_df.head(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#J. Guardamos el modelo.\n",
        "model_lgb.save_model('./lgb_datos_fe_05_train_solo_marzo_abril_con_ratios.txt')\n",
        "#model_lgb.save_model(modelos_path + 'lgb_datos_fe_02_ajusto_optuna.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#K. Volvemos a leer el modelo.\n",
        "#model_lgb = lgb.Booster(model_file=\"./lgb_datos_fe_05_train_solo_marzo_abril_con_ratios.txt\")\n",
        "#model_lgb = lgb.Booster(model_file=modelos_path + 'lgb_datos_fe_02_ajusto_optuna.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#a. Importo librería.\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "#b. Configura el API de Kaggle\n",
        "api = KaggleApi()\n",
        "api.authenticate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#L. Predecimos Junio.\n",
        "#i. Predecimos propiamente dicho.\n",
        "predicciones = model_lgb.predict(X_test)\n",
        "#ii. Le pegamos la probabilidad de ser \"BAJA\" a cada cliente.\n",
        "X_test['Probabilidad'] = predicciones\n",
        "#iii. Ordenamos a los clientes por probabilidad de ser \"BAJA\" de forma descendente.\n",
        "tb_entrega = X_test.sort_values(by='Probabilidad', ascending=False)\n",
        "#iv. Genero una lista de distintos cortes candidatos, para enviar a Kaggle.\n",
        "cortes = range(12400,15000,200)\n",
        "#v. Generamos las distintas predicciones de clases a partir de los distintos cortes posibles.\n",
        "num_subida_kaggle = 22\n",
        "for envios in cortes:\n",
        "    #1. Le ponemos clase 1 (\"BAJA\") a los primeros \"envios\" con mayor probabilidad.\n",
        "    tb_entrega['Predicted'] = 0\n",
        "    tb_entrega.iloc[:envios, tb_entrega.columns.get_loc('Predicted')] = 1\n",
        "    resultados = tb_entrega[[\"numero_de_cliente\", 'Predicted']].reset_index(drop=True)\n",
        "    \n",
        "    print(\"Cantidad de clientes {}\".format(envios))\n",
        "    #2. Guardamos el archivo para Kaggle.\n",
        "    nombre_archivo = \"K106_S5A_00{}.csv\".format(num_subida_kaggle)\n",
        "    ruta_archivo= \"../../../exp/{}\".format(nombre_archivo)\n",
        "    resultados.to_csv(ruta_archivo, index=False)\n",
        "    \n",
        "    num_subida_kaggle += 1\n",
        "    \n",
        "    #3. Envío a Kaggle.\n",
        "    #a. Defino los parámetros claves.\n",
        "    mensaje = f'Archivo {nombre_archivo}. LGB Train Marzo-Abril FE (lgb_datos_fe_05_train_solo_marzo_abril_con_ratios), punto_corte: {envios}. No se imputa, 100 Trials para búsqueda de hiperparámetros, 1000 boost con stop early'\n",
        "    competencia = 'dm-ey-f-2024-primera'\n",
        "    #c. Subo la Submission.\n",
        "    while True:\n",
        "        try:\n",
        "            api.competition_submit(file_name=ruta_archivo, message=mensaje, competition=competencia)\n",
        "            print(\"Submission successful!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            if e.status == 429:  # Too Many Requests\n",
        "                print(\"Rate limit exceeded. Retrying after 30 seconds...\")\n",
        "                time.sleep(30)\n",
        "            else:\n",
        "                raise e  # Re-raise other exceptions"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dmeyf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
